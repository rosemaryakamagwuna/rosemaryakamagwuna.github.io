{"status":"ok","feed":{"url":"https://medium.com/feed/@@jolomi-tosanwumi","title":"Stories by Jolomi Tosanwumi on Medium","link":"https://medium.com/@jolomi-tosanwumi?source=rss-9d28fafd6645------2","author":"","description":"Stories by Jolomi Tosanwumi on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*5msL-oHJgn8y309QS0INPA.jpeg"},"items":[{"title":"F-beta Score in Keras Part III","pubDate":"2020-11-30 23:45:08","link":"https://towardsdatascience.com/f-beta-score-in-keras-part-iii-28b1721fc442?source=rss-9d28fafd6645------2","guid":"https://medium.com/p/28b1721fc442","author":"Jolomi Tosanwumi","thumbnail":"","description":"\n<h4>Creating custom F2 score for multi-label classification problems in\u00a0Keras</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*RXUjQXDMmSXD2NxkDm-Rjg.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@theblowup?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">the blowup</a> on\u00a0<a href=\"https://unsplash.com/s/photos/multi-classification?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Monitoring f-beta score for multi-label classification in Keras is often desired by data scientist. Unfortunately, F-beta metrics was removed in <a href=\"https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes\">Keras 2.0</a> because it can be misleading when computed in batches rather than globally (for the whole dataset). It will be more misleading if the batch size is small or when a minority class has a very small number of observations. Sometimes, many data scientists are interested in knowing the F-beta score per batch for different reasons when the batch size is large. In this article, we will be implementing a custom f-beta function for multi-label classification in\u00a0Keras.</p>\n<p>In <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">part II</a> of this article, we implemented f-beta score in Keras for multiclass problem both as a stateful and stateless metric. We also saw the different ways of aggregating f-beta score for multiclass problem. In this article, we will be explaining how f-beta score can be applied to multi-label classification problem and creating both stateful and stateless custom f-beta metric for multi-label classification problem in Keras. We will assume you are familiar with the basics of deep learning, machine learning classifiers, and\u00a0NLP.</p>\n<p>In multi-label classification problems, we are predicting all the classes an observation belongs to. For example, predicting all the fruits present in an image from a set of fruits like apple, banana, orange, mango and cucumber. Worth noting in multi label classification is that an observation can belong to one or more classes at a time whether in the training or testing set. Also, the actual and predicted labels need to be one-hot encoded making them to be two dimensional arrays. Like in multiclass problem, metrics like f-beta score can be calculated per class before aggregating using either of micro, macro and weighted methods. Unlike to multiclass f-beta score, multi-label f-beta score could also be calculated per sample before aggregating the\u00a0results.</p>\n<p>Note: In this article, we will be using OneVsRest (OVR) strategy in explaining the per sample f-beta\u00a0score.</p>\n<h3>Per Sample</h3>\n<p>In per sample f-beta score, the f-beta score for the actual and predicted labels of each observation (sample) is calculated before aggregation. The diagram below helps in understanding how this is\u00a0done.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/334/1*4q8K_Xxr5h7trgkgbRnJKQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>Each actual and prediction label of each sample in multi-label classification is an array that contains 1 or 0, and can be thought of as y_true and y_pred of a binary classification problem. Therefore, y_true and y_pred of multi-label classification can be seen as a horizontal stack of separate binary classification y_true and y_pred respectively. We therefore compute the f-beta score for each sample and aggregate them as our final result. The aggregation can be weighted or not. For simplicity, we will be focusing more on the unweighted aggregation of per sample f-beta score in this article. We will implement it as both stateless and stateful\u00a0metric.</p>\n<h3>Stateless F-beta</h3>\n<p>As explained in part I of this article, stateless metric according to <a href=\"https://keras.io/api/metrics/#as-simple-callables-stateless\">Keras documentation</a> means that the metric is estimated per batch. Therefore, the last metric reported after training is actually that of the last batch. Sometimes, we may want to monitor a metric per batch during training especially when the batch size is large, validation data size is the expected test size or due to the fact that weights of nodes are updated per batch. To demonstrate how to implement this in Keras, we will generate a multi-label dataset using Scikit-learn\u2019s <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_multilabel_classification.html\"><em>make_multilabel_classification </em></a>function.</p>\n<p>First, we import useful libraries.</p>\n<a href=\"https://medium.com/media/2c311ad9db7dc62d5a8a4c2600e1805e/href\">https://medium.com/media/2c311ad9db7dc62d5a8a4c2600e1805e/href</a><p>Our generated dataset can be thought of as a <a href=\"https://machinelearningmastery.com/gentle-introduction-bag-words-model/\">Bag of Words</a> (BOW) document vectors which we will transform using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">TfidfTransformer</a>.</p>\n<a href=\"https://medium.com/media/9bab229b30e790756faff76d5b92add6/href\">https://medium.com/media/9bab229b30e790756faff76d5b92add6/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/226/1*rdDwJzSxSkpCseNr0DQqnw.png\"></figure><p>In part I of this article, we calculated the f1 score during training using Scikit-learn\u2019s fbeta_score function after setting the <em>run_eagerly </em>parameter of the compile method of our Keras sequential model to False. We also observed that this method is slower than using functions wrapped in Tensorflow\u2019s <em>tf.function </em>logic<em>. </em>In this article, we will go straight to defining a custom f-beta score function wrapped in Tensorflow\u2019s <em>tf.function </em>logic that wouldn\u2019t be run eagerly for brevity. We will simply call this function <em>multi_label_fbeta. </em>Implementation of this function will be possible based on the facts that for <em>ytrue </em>and <em>ypred </em>arrays of a multi-label problem where 1 is positive and 0 is negative:</p>\n<ol>\n<li>True positive is the sum of the element-wise multiplication of the two\u00a0arrays.</li>\n<li>Predicted positive is the sum of\u00a0<em>ypred.</em>\n</li>\n<li>Actual positive is the sum of\u00a0<em>ytrue.</em>\n</li>\n</ol>\n<a href=\"https://medium.com/media/a7205bdbd7230770ef8657606da09dbf/href\">https://medium.com/media/a7205bdbd7230770ef8657606da09dbf/href</a><p>Our aim is not to build a high performance model but to demonstrate how to monitor f-beta score in multi-label classification in Keras. For this reason, we will build a simple model that is quick to train and will run for few\u00a0epochs.</p>\n<a href=\"https://medium.com/media/eaedd44af3cf1005ab694c15da25e60f/href\">https://medium.com/media/eaedd44af3cf1005ab694c15da25e60f/href</a><a href=\"https://medium.com/media/46f56cf088930a1ac7fcee30b6145873/href\">https://medium.com/media/46f56cf088930a1ac7fcee30b6145873/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ijeC7900XZbkjQF-k-jM3Q.png\"></figure><p>We will now test the rightness of our multi-label f-beta function.</p>\n<a href=\"https://medium.com/media/695cc2d6268ef802cd2ab677ce4e4be4/href\">https://medium.com/media/695cc2d6268ef802cd2ab677ce4e4be4/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/550/1*A7UkfNAGRY-j8F0EAcaScA.png\"></figure><h3>Stateful F-beta\u00a0Score</h3>\n<p>When we are not interested in the per batch metric but in the metric evaluated on the whole dataset, we need to subclass the <em>Metric </em>class so that a state is maintained across all batches. This maintained state is made possible by keeping track of variables (called state variables) that are useful in evaluating our metric across all batches. When this happens, our metric is said to be stateful (you could set verbose to 2 in the model\u2019s fit method so as to report only the metric of the last batch which is that of the whole dataset for stateful metrics). According to <a href=\"https://keras.io/api/metrics/#as-subclasses-of-metric-stateful\">Keras documentation</a>, there are four <em>methods </em>a stateful metric should\u00a0have:</p>\n<ol>\n<li>__init__\u00a0: we create (initialize) the state variables here.</li>\n<li>update: this method is called at the end of each batch and is used to change (update) the state variables.</li>\n<li>result: this is called at the end of each batch after states variables are updated. It is used to compute and return the metric for each\u00a0batch.</li>\n<li>reset: this is called at the end of each epoch. It is used to clear (reinitialize) the state variables.</li>\n</ol>\n<p>For multi-label f-beta metric, state variables would definitely be true positives, actual positives, predicted positives, number of samples and sum of f-beta scores because they can easily be tracked across all batches. Let\u2019s now implement a stateful f-beta metric for our multi-label problem.</p>\n<a href=\"https://medium.com/media/fe87eb21f6e5bffffd4a14d6ce2e8bc0/href\">https://medium.com/media/fe87eb21f6e5bffffd4a14d6ce2e8bc0/href</a><p>Training our\u00a0model</p>\n<a href=\"https://medium.com/media/831e636f7515889ab65902f2ea6d14ab/href\">https://medium.com/media/831e636f7515889ab65902f2ea6d14ab/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CwVYDDGsIF82kuVayVM6IA.png\"></figure><p>Finally, we will check the rightness of our stateful f-beta by comparing it with Scikit-learn\u2019s f-beta score metric on some randomly generated multi-label ytrue and\u00a0ypred.</p>\n<a href=\"https://medium.com/media/9733ca9f20d89e863112d10bb8617e60/href\">https://medium.com/media/9733ca9f20d89e863112d10bb8617e60/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/580/1*SAWBpy7yhfzvQtkf-OaZGg.png\"></figure><h3>Conclusion</h3>\n<p>F-beta score can be implemented in Keras for multi-label problem either as a stateful or a stateless metric as we have seen in this article. We have also seen the different ways of aggregating f-beta score for multi-label problem. See all codes in my <a href=\"https://github.com/Jolomi-Tosanwumi/F-beta-Score-in-Keras\">GitHub repository</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=28b1721fc442\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-iii-28b1721fc442\">F-beta Score in Keras Part III</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Creating custom F2 score for multi-label classification problems in\u00a0Keras</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*RXUjQXDMmSXD2NxkDm-Rjg.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@theblowup?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">the blowup</a> on\u00a0<a href=\"https://unsplash.com/s/photos/multi-classification?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Monitoring f-beta score for multi-label classification in Keras is often desired by data scientist. Unfortunately, F-beta metrics was removed in <a href=\"https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes\">Keras 2.0</a> because it can be misleading when computed in batches rather than globally (for the whole dataset). It will be more misleading if the batch size is small or when a minority class has a very small number of observations. Sometimes, many data scientists are interested in knowing the F-beta score per batch for different reasons when the batch size is large. In this article, we will be implementing a custom f-beta function for multi-label classification in\u00a0Keras.</p>\n<p>In <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">part II</a> of this article, we implemented f-beta score in Keras for multiclass problem both as a stateful and stateless metric. We also saw the different ways of aggregating f-beta score for multiclass problem. In this article, we will be explaining how f-beta score can be applied to multi-label classification problem and creating both stateful and stateless custom f-beta metric for multi-label classification problem in Keras. We will assume you are familiar with the basics of deep learning, machine learning classifiers, and\u00a0NLP.</p>\n<p>In multi-label classification problems, we are predicting all the classes an observation belongs to. For example, predicting all the fruits present in an image from a set of fruits like apple, banana, orange, mango and cucumber. Worth noting in multi label classification is that an observation can belong to one or more classes at a time whether in the training or testing set. Also, the actual and predicted labels need to be one-hot encoded making them to be two dimensional arrays. Like in multiclass problem, metrics like f-beta score can be calculated per class before aggregating using either of micro, macro and weighted methods. Unlike to multiclass f-beta score, multi-label f-beta score could also be calculated per sample before aggregating the\u00a0results.</p>\n<p>Note: In this article, we will be using OneVsRest (OVR) strategy in explaining the per sample f-beta\u00a0score.</p>\n<h3>Per Sample</h3>\n<p>In per sample f-beta score, the f-beta score for the actual and predicted labels of each observation (sample) is calculated before aggregation. The diagram below helps in understanding how this is\u00a0done.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/334/1*4q8K_Xxr5h7trgkgbRnJKQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>Each actual and prediction label of each sample in multi-label classification is an array that contains 1 or 0, and can be thought of as y_true and y_pred of a binary classification problem. Therefore, y_true and y_pred of multi-label classification can be seen as a horizontal stack of separate binary classification y_true and y_pred respectively. We therefore compute the f-beta score for each sample and aggregate them as our final result. The aggregation can be weighted or not. For simplicity, we will be focusing more on the unweighted aggregation of per sample f-beta score in this article. We will implement it as both stateless and stateful\u00a0metric.</p>\n<h3>Stateless F-beta</h3>\n<p>As explained in part I of this article, stateless metric according to <a href=\"https://keras.io/api/metrics/#as-simple-callables-stateless\">Keras documentation</a> means that the metric is estimated per batch. Therefore, the last metric reported after training is actually that of the last batch. Sometimes, we may want to monitor a metric per batch during training especially when the batch size is large, validation data size is the expected test size or due to the fact that weights of nodes are updated per batch. To demonstrate how to implement this in Keras, we will generate a multi-label dataset using Scikit-learn\u2019s <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_multilabel_classification.html\"><em>make_multilabel_classification </em></a>function.</p>\n<p>First, we import useful libraries.</p>\n<a href=\"https://medium.com/media/2c311ad9db7dc62d5a8a4c2600e1805e/href\">https://medium.com/media/2c311ad9db7dc62d5a8a4c2600e1805e/href</a><p>Our generated dataset can be thought of as a <a href=\"https://machinelearningmastery.com/gentle-introduction-bag-words-model/\">Bag of Words</a> (BOW) document vectors which we will transform using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">TfidfTransformer</a>.</p>\n<a href=\"https://medium.com/media/9bab229b30e790756faff76d5b92add6/href\">https://medium.com/media/9bab229b30e790756faff76d5b92add6/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/226/1*rdDwJzSxSkpCseNr0DQqnw.png\"></figure><p>In part I of this article, we calculated the f1 score during training using Scikit-learn\u2019s fbeta_score function after setting the <em>run_eagerly </em>parameter of the compile method of our Keras sequential model to False. We also observed that this method is slower than using functions wrapped in Tensorflow\u2019s <em>tf.function </em>logic<em>. </em>In this article, we will go straight to defining a custom f-beta score function wrapped in Tensorflow\u2019s <em>tf.function </em>logic that wouldn\u2019t be run eagerly for brevity. We will simply call this function <em>multi_label_fbeta. </em>Implementation of this function will be possible based on the facts that for <em>ytrue </em>and <em>ypred </em>arrays of a multi-label problem where 1 is positive and 0 is negative:</p>\n<ol>\n<li>True positive is the sum of the element-wise multiplication of the two\u00a0arrays.</li>\n<li>Predicted positive is the sum of\u00a0<em>ypred.</em>\n</li>\n<li>Actual positive is the sum of\u00a0<em>ytrue.</em>\n</li>\n</ol>\n<a href=\"https://medium.com/media/a7205bdbd7230770ef8657606da09dbf/href\">https://medium.com/media/a7205bdbd7230770ef8657606da09dbf/href</a><p>Our aim is not to build a high performance model but to demonstrate how to monitor f-beta score in multi-label classification in Keras. For this reason, we will build a simple model that is quick to train and will run for few\u00a0epochs.</p>\n<a href=\"https://medium.com/media/eaedd44af3cf1005ab694c15da25e60f/href\">https://medium.com/media/eaedd44af3cf1005ab694c15da25e60f/href</a><a href=\"https://medium.com/media/46f56cf088930a1ac7fcee30b6145873/href\">https://medium.com/media/46f56cf088930a1ac7fcee30b6145873/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ijeC7900XZbkjQF-k-jM3Q.png\"></figure><p>We will now test the rightness of our multi-label f-beta function.</p>\n<a href=\"https://medium.com/media/695cc2d6268ef802cd2ab677ce4e4be4/href\">https://medium.com/media/695cc2d6268ef802cd2ab677ce4e4be4/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/550/1*A7UkfNAGRY-j8F0EAcaScA.png\"></figure><h3>Stateful F-beta\u00a0Score</h3>\n<p>When we are not interested in the per batch metric but in the metric evaluated on the whole dataset, we need to subclass the <em>Metric </em>class so that a state is maintained across all batches. This maintained state is made possible by keeping track of variables (called state variables) that are useful in evaluating our metric across all batches. When this happens, our metric is said to be stateful (you could set verbose to 2 in the model\u2019s fit method so as to report only the metric of the last batch which is that of the whole dataset for stateful metrics). According to <a href=\"https://keras.io/api/metrics/#as-subclasses-of-metric-stateful\">Keras documentation</a>, there are four <em>methods </em>a stateful metric should\u00a0have:</p>\n<ol>\n<li>__init__\u00a0: we create (initialize) the state variables here.</li>\n<li>update: this method is called at the end of each batch and is used to change (update) the state variables.</li>\n<li>result: this is called at the end of each batch after states variables are updated. It is used to compute and return the metric for each\u00a0batch.</li>\n<li>reset: this is called at the end of each epoch. It is used to clear (reinitialize) the state variables.</li>\n</ol>\n<p>For multi-label f-beta metric, state variables would definitely be true positives, actual positives, predicted positives, number of samples and sum of f-beta scores because they can easily be tracked across all batches. Let\u2019s now implement a stateful f-beta metric for our multi-label problem.</p>\n<a href=\"https://medium.com/media/fe87eb21f6e5bffffd4a14d6ce2e8bc0/href\">https://medium.com/media/fe87eb21f6e5bffffd4a14d6ce2e8bc0/href</a><p>Training our\u00a0model</p>\n<a href=\"https://medium.com/media/831e636f7515889ab65902f2ea6d14ab/href\">https://medium.com/media/831e636f7515889ab65902f2ea6d14ab/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CwVYDDGsIF82kuVayVM6IA.png\"></figure><p>Finally, we will check the rightness of our stateful f-beta by comparing it with Scikit-learn\u2019s f-beta score metric on some randomly generated multi-label ytrue and\u00a0ypred.</p>\n<a href=\"https://medium.com/media/9733ca9f20d89e863112d10bb8617e60/href\">https://medium.com/media/9733ca9f20d89e863112d10bb8617e60/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/580/1*SAWBpy7yhfzvQtkf-OaZGg.png\"></figure><h3>Conclusion</h3>\n<p>F-beta score can be implemented in Keras for multi-label problem either as a stateful or a stateless metric as we have seen in this article. We have also seen the different ways of aggregating f-beta score for multi-label problem. See all codes in my <a href=\"https://github.com/Jolomi-Tosanwumi/F-beta-Score-in-Keras\">GitHub repository</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=28b1721fc442\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-iii-28b1721fc442\">F-beta Score in Keras Part III</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["f1-score","f1-score-keras","keras","fbeta-score"]},{"title":"F-beta Score in Keras Part II","pubDate":"2020-11-30 20:28:33","link":"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4?source=rss-9d28fafd6645------2","guid":"https://medium.com/p/15f91f07c9a4","author":"Jolomi Tosanwumi","thumbnail":"","description":"\n<h4>Creating custom F1 score for multi classification problems in\u00a0Keras</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/639/1*erBqVlhtvIHnFhzLovQriA.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@echaparro?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Edgar Chaparro</a> on\u00a0<a href=\"https://unsplash.com/s/photos/multi-classification?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>In the <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-i-86ad190a252f\">previous article</a> (part I), we explained stateless and stateful metrics in Keras, derived the formula for f-beta score and created both stateless and stateful custom f-beta metric in Keras for binary classification problems. In this article (part II), we will be explaining how f-beta score can be applied to multi-classification problems. We will also create both stateful and stateless custom f-beta metric for multi classification problems in Keras. We will assume you are familiar with the basics of deep learning and machine learning classifiers.</p>\n<p>In multi classification problems, we are predicting if an observation belongs to one of a given set of three or more classes. For example, predicting if an image is that of a cat, dog, fish or a bird. Worth noting in multi classification is that an observation can only belong to one and only one class at a time whether in the training or testing set. Also, the actual and predicted labels need to be one-hot encoded making them to be two dimensional arrays. The same logic for f-beta in binary classification applies to multi classification with just some few adjustments. For instance, the f-beta score could be calculated per class or per sample before aggregating the results. In multiclass problem, calculating f-beta score per sample is highly discouraged because its true positive, false positive and false negative can only have a value of either zero or one due to the fact that only one class is predicted per sample. For example, if our model correctly predict the class of a particular sample, true positive will be 1, while false positive and false negative will both be 0 for that sample. If the model wrongly classify a sample, true positive will be 0, false positive and false negative will both be 1. This will make recall and precision equal for each sample and limit their values to either be 0 or infinity. In fact, we shouldn\u2019t compute the f-beta score for multiclass problem per sample, this method is only safe for multi label problem which we will see in part III of this\u00a0article.</p>\n<p>We will now focus on multiclass f-beta computed per class. There are three ways of aggregating multiclass f-beta computed per class and they\u00a0are:</p>\n<ol>\n<li>Micro</li>\n<li>Macro</li>\n<li>Weighted</li>\n</ol>\n<p>Note: In this article, we will be using OneVsRest (OVR) strategy in explaining the aggregation methods\u00a0above.</p>\n<h3>Micro</h3>\n<p>In micro mode, we compute the f-beta score globally by finding the harmonic mean of the global precision and recall. For example, let\u2019s consider the confusion matrix for a multiclass problem as shown\u00a0below:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/212/1*WBkEHIIJniEBN3Zc0adhvw.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>The global true positive is 600+250+40 which equals 890, while the global false positive is 50+150+200+50+70+90 which equals 610. It is important to note that the global false positives can also be considered as global false negatives meaning global false negatives equals 610 also. This implies that the global precision which is 0.593 is same as the global recall. We know that if precision and recall are the same, f-beta will be of same value. Therefore, global f-beta is also 0.593 irrespective of the value of beta. If we were to calculate the accuracy from the confusion matrix above, we will still get 0.593 because what we call global true positive is actually the total number of correct predictions, while global false positive (or global false negative) is actually the total number of wrong predictions. In fact, in micro averaging:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/304/1*t7kUPTsIaFWuRE9HBvAANA.gif\"></figure><h3>Macro</h3>\n<p>In macro mode, metrics are calculated along axis 0. For example, in the diagram below, we have three classes A, B, and C; we compute the metric say f-beta for y_true and y_pred of each class. This is done as though y_true and y_pred of each class were those of binary classification problem.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/319/1*Pa4BFSK4Uc1Ce8SKSYcEaw.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>In our example above, we will obtain three results which will result in a need of averaging the results. When using the macro averaging, we simply take the mean of the\u00a0results.</p>\n<h3>Weighted averaging</h3>\n<p>Weighted averaging is similar to macro averaging except that the weighted mean of the f-beta scores is return. This considers class imbalance and is done by taking the summation of the products of the f-beta score and the sample fraction of each class. Mathematically, it can be written\u00a0as:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/145/1*viIW9deTdEl-5X6NdsnN6g.gif\"></figure><p>where Wf\u1d66 is the weighted\u00a0f\u1d66,</p>\n<p>p\u1d62 is the probability of choosing a class (sample fraction of the class in\u00a0ytrue),</p>\n<p>(f\u1d66)\u1d62 is the f-beta for a particular class</p>\n<p>n is the number of\u00a0classes</p>\n<p>Haven understood the different methods of aggregating multiclass f-beta score, we will now proceed to implementing them in python specifically macro and weighted f-beta as both stateful and stateless f-beta score. For brevity, we will not be implementing micro multiclass f-beta because it is the same as accuracy.</p>\n<h3>Stateless F-beta</h3>\n<p>As explained in part I of this article, stateless metric according to <a href=\"https://keras.io/api/metrics/#as-simple-callables-stateless\">Keras documentation</a> means that the metric is estimated per batch. Therefore, the last metric reported after training is actually that of the last batch. Sometimes, we may want to monitor a metric per batch during training especially when the batch size is large, validation data size is the expected test size or due to the fact that weights of nodes are updated per batch. To demonstrate how to implement this in Keras, we will be using the famous Modified National Institute of Standards and Technology (MNIST) <a href=\"https://en.wikipedia.org/wiki/MNIST_database#:~:text=The%20MNIST%20database%20(%20National%20Institute,the%20field%20of%20machine%20learning.\">dataset </a>which is a dataset of 60,000 training and 10,000 testing 28x28 grayscale images of handwritten digits between 0 and 9 (inclusive). First, let\u2019s import useful libraries.</p>\n<a href=\"https://medium.com/media/6a520c4a11d451c497c3c25ea95ffa6d/href\">https://medium.com/media/6a520c4a11d451c497c3c25ea95ffa6d/href</a><p>Let\u2019s download the\u00a0dataset.</p>\n<a href=\"https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href\">https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*s0fPhKuPijZ0BcvpCtVjQA.png\"></figure><p>Let\u2019s randomly view some of the images and their corresponding labels.</p>\n<a href=\"https://medium.com/media/281ee17ebef35194d55f018d81a62568/href\">https://medium.com/media/281ee17ebef35194d55f018d81a62568/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/344/1*t50SUMwXCxIY0QN-EgBMTQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><a href=\"https://medium.com/media/340f62cae811c8d189b6c67eb8085b50/href\">https://medium.com/media/340f62cae811c8d189b6c67eb8085b50/href</a><p>In part I of this article, we calculated the f1 score during training using Scikit-learn\u2019s fbeta_score function after setting the <em>run_eagerly </em>parameter of the compile method of our Keras sequential model to False. We also observed that this method is slower than using functions wrapped in Tensorflow\u2019s <em>tf.function </em>logic<em>. </em>In this article, we will go straight to defining a custom f-beta score function wrapped in Tensorflow\u2019s <em>tf.function </em>logic that wouldn\u2019t be run eagerly for brevity. We will simply call this function <em>multi_class_fbeta. </em>Implementation of this function will be possible based on the facts that for <em>ytrue </em>and <em>ypred </em>arrays of a multiclass problem where 1 is positive and 0 is negative:</p>\n<ol>\n<li>True positive is the sum of the element-wise multiplication of the two\u00a0arrays.</li>\n<li>Predicted positive is the sum of\u00a0<em>ypred.</em>\n</li>\n<li>Actual positive is the sum of\u00a0<em>ytrue.</em>\n</li>\n</ol>\n<a href=\"https://medium.com/media/a9a42c33d8a8d792980ca13cf196eadf/href\">https://medium.com/media/a9a42c33d8a8d792980ca13cf196eadf/href</a><p>We will now define a function to build our model. The aim of this article is to demonstrate how to create custom f-beta score metric and not to build a high performance model. So, we will build a simple convolutional neural network which will run for few\u00a0epochs.</p>\n<a href=\"https://medium.com/media/1edff9d59355a0a0b96c010de93e69e7/href\">https://medium.com/media/1edff9d59355a0a0b96c010de93e69e7/href</a><p>We now train the model on the training\u00a0set.</p>\n<a href=\"https://medium.com/media/66220a29a23261895b04fbc8d21eb170/href\">https://medium.com/media/66220a29a23261895b04fbc8d21eb170/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*O8RtjGnmaJbSwMiID9YpUQ.png\"></figure><p>Let\u2019s confirm the rightness of our custom f-beta function by comparing its evaluation of the testing set to that of Scikit-learn\u2019s f-beta function.</p>\n<a href=\"https://medium.com/media/175561e1cf3e5634ce0564619974836a/href\">https://medium.com/media/175561e1cf3e5634ce0564619974836a/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/812/1*RYX-Ve9pdaOip366U_9hKA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*bW7TPyAiKeJhkOhj5FCjvw.png\"><figcaption>Image by\u00a0author</figcaption></figure><h3>Stateful F-beta</h3>\n<p>When we are not interested in the per batch metric but in the metric evaluated on the whole dataset, we need to subclass the <em>Metric </em>class so that a state is maintained across all batches. This maintained state is made possible by keeping track of variables (called state variables) that are useful in evaluating our metric across all batches. When this happens, our metric is said to be stateful (you could set verbose to 2 in the model\u2019s fit method so as to report only the metric of the last batch which is that of the whole dataset for stateful metrics). According to <a href=\"https://keras.io/api/metrics/#as-subclasses-of-metric-stateful\">Keras documentation</a>, there are four <em>methods </em>a stateful metric should\u00a0have:</p>\n<ol>\n<li>__init__\u00a0: we create (initialize) the state variables here.</li>\n<li>update: this method is called at the end of each batch and is used to change (update) the state variables.</li>\n<li>result: this is called at the end of each batch after states variables are updated. It is used to compute and return the metric for each\u00a0batch.</li>\n<li>reset: this is called at the end of each epoch. It is used to clear (reinitialize) the state variables.</li>\n</ol>\n<p>For multiclass f-beta metric, state variables would definitely be true positives, actual positives and predicted positives because they can easily be tracked across all batches. Let\u2019s now implement a stateful f-beta metric for our multiclass problem.</p>\n<a href=\"https://medium.com/media/ec424f910d66af1f8e12f6f146a5cc5c/href\">https://medium.com/media/ec424f910d66af1f8e12f6f146a5cc5c/href</a><a href=\"https://medium.com/media/ee9ba253866477b5acdf59d3a817f54e/href\">https://medium.com/media/ee9ba253866477b5acdf59d3a817f54e/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AsEf6wiAU6o3_AdZG5CSpA.png\"></figure><p>Finally, we will check the rightness of our stateful f-beta by comparing it with Scikit-learn\u2019s f-beta score metric on some randomly generated multiclass ytrue and\u00a0ypred.</p>\n<a href=\"https://medium.com/media/6543c1ca91ee56701ea937fdf5e2f6fc/href\">https://medium.com/media/6543c1ca91ee56701ea937fdf5e2f6fc/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/562/1*UKBJ_t7Yk2lShHHXCIncuQ.png\"></figure><h3>Conclusion</h3>\n<p>F-beta score can be implemented in Keras for multiclass problem either as a stateful or a stateless metric as we have seen in this article. We have also seen the different ways of aggregating f-beta score for multiclass problem. In <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-iii-28b1721fc442\">part III</a>, we will be implementing the f-beta score for multi-label classification problems. See all codes in my <a href=\"https://github.com/Jolomi-Tosanwumi/F-beta-Score-in-Keras\">GitHub repository</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=15f91f07c9a4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">F-beta Score in Keras Part II</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Creating custom F1 score for multi classification problems in\u00a0Keras</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/639/1*erBqVlhtvIHnFhzLovQriA.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@echaparro?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Edgar Chaparro</a> on\u00a0<a href=\"https://unsplash.com/s/photos/multi-classification?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>In the <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-i-86ad190a252f\">previous article</a> (part I), we explained stateless and stateful metrics in Keras, derived the formula for f-beta score and created both stateless and stateful custom f-beta metric in Keras for binary classification problems. In this article (part II), we will be explaining how f-beta score can be applied to multi-classification problems. We will also create both stateful and stateless custom f-beta metric for multi classification problems in Keras. We will assume you are familiar with the basics of deep learning and machine learning classifiers.</p>\n<p>In multi classification problems, we are predicting if an observation belongs to one of a given set of three or more classes. For example, predicting if an image is that of a cat, dog, fish or a bird. Worth noting in multi classification is that an observation can only belong to one and only one class at a time whether in the training or testing set. Also, the actual and predicted labels need to be one-hot encoded making them to be two dimensional arrays. The same logic for f-beta in binary classification applies to multi classification with just some few adjustments. For instance, the f-beta score could be calculated per class or per sample before aggregating the results. In multiclass problem, calculating f-beta score per sample is highly discouraged because its true positive, false positive and false negative can only have a value of either zero or one due to the fact that only one class is predicted per sample. For example, if our model correctly predict the class of a particular sample, true positive will be 1, while false positive and false negative will both be 0 for that sample. If the model wrongly classify a sample, true positive will be 0, false positive and false negative will both be 1. This will make recall and precision equal for each sample and limit their values to either be 0 or infinity. In fact, we shouldn\u2019t compute the f-beta score for multiclass problem per sample, this method is only safe for multi label problem which we will see in part III of this\u00a0article.</p>\n<p>We will now focus on multiclass f-beta computed per class. There are three ways of aggregating multiclass f-beta computed per class and they\u00a0are:</p>\n<ol>\n<li>Micro</li>\n<li>Macro</li>\n<li>Weighted</li>\n</ol>\n<p>Note: In this article, we will be using OneVsRest (OVR) strategy in explaining the aggregation methods\u00a0above.</p>\n<h3>Micro</h3>\n<p>In micro mode, we compute the f-beta score globally by finding the harmonic mean of the global precision and recall. For example, let\u2019s consider the confusion matrix for a multiclass problem as shown\u00a0below:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/212/1*WBkEHIIJniEBN3Zc0adhvw.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>The global true positive is 600+250+40 which equals 890, while the global false positive is 50+150+200+50+70+90 which equals 610. It is important to note that the global false positives can also be considered as global false negatives meaning global false negatives equals 610 also. This implies that the global precision which is 0.593 is same as the global recall. We know that if precision and recall are the same, f-beta will be of same value. Therefore, global f-beta is also 0.593 irrespective of the value of beta. If we were to calculate the accuracy from the confusion matrix above, we will still get 0.593 because what we call global true positive is actually the total number of correct predictions, while global false positive (or global false negative) is actually the total number of wrong predictions. In fact, in micro averaging:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/304/1*t7kUPTsIaFWuRE9HBvAANA.gif\"></figure><h3>Macro</h3>\n<p>In macro mode, metrics are calculated along axis 0. For example, in the diagram below, we have three classes A, B, and C; we compute the metric say f-beta for y_true and y_pred of each class. This is done as though y_true and y_pred of each class were those of binary classification problem.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/319/1*Pa4BFSK4Uc1Ce8SKSYcEaw.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>In our example above, we will obtain three results which will result in a need of averaging the results. When using the macro averaging, we simply take the mean of the\u00a0results.</p>\n<h3>Weighted averaging</h3>\n<p>Weighted averaging is similar to macro averaging except that the weighted mean of the f-beta scores is return. This considers class imbalance and is done by taking the summation of the products of the f-beta score and the sample fraction of each class. Mathematically, it can be written\u00a0as:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/145/1*viIW9deTdEl-5X6NdsnN6g.gif\"></figure><p>where Wf\u1d66 is the weighted\u00a0f\u1d66,</p>\n<p>p\u1d62 is the probability of choosing a class (sample fraction of the class in\u00a0ytrue),</p>\n<p>(f\u1d66)\u1d62 is the f-beta for a particular class</p>\n<p>n is the number of\u00a0classes</p>\n<p>Haven understood the different methods of aggregating multiclass f-beta score, we will now proceed to implementing them in python specifically macro and weighted f-beta as both stateful and stateless f-beta score. For brevity, we will not be implementing micro multiclass f-beta because it is the same as accuracy.</p>\n<h3>Stateless F-beta</h3>\n<p>As explained in part I of this article, stateless metric according to <a href=\"https://keras.io/api/metrics/#as-simple-callables-stateless\">Keras documentation</a> means that the metric is estimated per batch. Therefore, the last metric reported after training is actually that of the last batch. Sometimes, we may want to monitor a metric per batch during training especially when the batch size is large, validation data size is the expected test size or due to the fact that weights of nodes are updated per batch. To demonstrate how to implement this in Keras, we will be using the famous Modified National Institute of Standards and Technology (MNIST) <a href=\"https://en.wikipedia.org/wiki/MNIST_database#:~:text=The%20MNIST%20database%20(%20National%20Institute,the%20field%20of%20machine%20learning.\">dataset </a>which is a dataset of 60,000 training and 10,000 testing 28x28 grayscale images of handwritten digits between 0 and 9 (inclusive). First, let\u2019s import useful libraries.</p>\n<a href=\"https://medium.com/media/6a520c4a11d451c497c3c25ea95ffa6d/href\">https://medium.com/media/6a520c4a11d451c497c3c25ea95ffa6d/href</a><p>Let\u2019s download the\u00a0dataset.</p>\n<a href=\"https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href\">https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*s0fPhKuPijZ0BcvpCtVjQA.png\"></figure><p>Let\u2019s randomly view some of the images and their corresponding labels.</p>\n<a href=\"https://medium.com/media/281ee17ebef35194d55f018d81a62568/href\">https://medium.com/media/281ee17ebef35194d55f018d81a62568/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/344/1*t50SUMwXCxIY0QN-EgBMTQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><a href=\"https://medium.com/media/340f62cae811c8d189b6c67eb8085b50/href\">https://medium.com/media/340f62cae811c8d189b6c67eb8085b50/href</a><p>In part I of this article, we calculated the f1 score during training using Scikit-learn\u2019s fbeta_score function after setting the <em>run_eagerly </em>parameter of the compile method of our Keras sequential model to False. We also observed that this method is slower than using functions wrapped in Tensorflow\u2019s <em>tf.function </em>logic<em>. </em>In this article, we will go straight to defining a custom f-beta score function wrapped in Tensorflow\u2019s <em>tf.function </em>logic that wouldn\u2019t be run eagerly for brevity. We will simply call this function <em>multi_class_fbeta. </em>Implementation of this function will be possible based on the facts that for <em>ytrue </em>and <em>ypred </em>arrays of a multiclass problem where 1 is positive and 0 is negative:</p>\n<ol>\n<li>True positive is the sum of the element-wise multiplication of the two\u00a0arrays.</li>\n<li>Predicted positive is the sum of\u00a0<em>ypred.</em>\n</li>\n<li>Actual positive is the sum of\u00a0<em>ytrue.</em>\n</li>\n</ol>\n<a href=\"https://medium.com/media/a9a42c33d8a8d792980ca13cf196eadf/href\">https://medium.com/media/a9a42c33d8a8d792980ca13cf196eadf/href</a><p>We will now define a function to build our model. The aim of this article is to demonstrate how to create custom f-beta score metric and not to build a high performance model. So, we will build a simple convolutional neural network which will run for few\u00a0epochs.</p>\n<a href=\"https://medium.com/media/1edff9d59355a0a0b96c010de93e69e7/href\">https://medium.com/media/1edff9d59355a0a0b96c010de93e69e7/href</a><p>We now train the model on the training\u00a0set.</p>\n<a href=\"https://medium.com/media/66220a29a23261895b04fbc8d21eb170/href\">https://medium.com/media/66220a29a23261895b04fbc8d21eb170/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*O8RtjGnmaJbSwMiID9YpUQ.png\"></figure><p>Let\u2019s confirm the rightness of our custom f-beta function by comparing its evaluation of the testing set to that of Scikit-learn\u2019s f-beta function.</p>\n<a href=\"https://medium.com/media/175561e1cf3e5634ce0564619974836a/href\">https://medium.com/media/175561e1cf3e5634ce0564619974836a/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/812/1*RYX-Ve9pdaOip366U_9hKA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*bW7TPyAiKeJhkOhj5FCjvw.png\"><figcaption>Image by\u00a0author</figcaption></figure><h3>Stateful F-beta</h3>\n<p>When we are not interested in the per batch metric but in the metric evaluated on the whole dataset, we need to subclass the <em>Metric </em>class so that a state is maintained across all batches. This maintained state is made possible by keeping track of variables (called state variables) that are useful in evaluating our metric across all batches. When this happens, our metric is said to be stateful (you could set verbose to 2 in the model\u2019s fit method so as to report only the metric of the last batch which is that of the whole dataset for stateful metrics). According to <a href=\"https://keras.io/api/metrics/#as-subclasses-of-metric-stateful\">Keras documentation</a>, there are four <em>methods </em>a stateful metric should\u00a0have:</p>\n<ol>\n<li>__init__\u00a0: we create (initialize) the state variables here.</li>\n<li>update: this method is called at the end of each batch and is used to change (update) the state variables.</li>\n<li>result: this is called at the end of each batch after states variables are updated. It is used to compute and return the metric for each\u00a0batch.</li>\n<li>reset: this is called at the end of each epoch. It is used to clear (reinitialize) the state variables.</li>\n</ol>\n<p>For multiclass f-beta metric, state variables would definitely be true positives, actual positives and predicted positives because they can easily be tracked across all batches. Let\u2019s now implement a stateful f-beta metric for our multiclass problem.</p>\n<a href=\"https://medium.com/media/ec424f910d66af1f8e12f6f146a5cc5c/href\">https://medium.com/media/ec424f910d66af1f8e12f6f146a5cc5c/href</a><a href=\"https://medium.com/media/ee9ba253866477b5acdf59d3a817f54e/href\">https://medium.com/media/ee9ba253866477b5acdf59d3a817f54e/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AsEf6wiAU6o3_AdZG5CSpA.png\"></figure><p>Finally, we will check the rightness of our stateful f-beta by comparing it with Scikit-learn\u2019s f-beta score metric on some randomly generated multiclass ytrue and\u00a0ypred.</p>\n<a href=\"https://medium.com/media/6543c1ca91ee56701ea937fdf5e2f6fc/href\">https://medium.com/media/6543c1ca91ee56701ea937fdf5e2f6fc/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/562/1*UKBJ_t7Yk2lShHHXCIncuQ.png\"></figure><h3>Conclusion</h3>\n<p>F-beta score can be implemented in Keras for multiclass problem either as a stateful or a stateless metric as we have seen in this article. We have also seen the different ways of aggregating f-beta score for multiclass problem. In <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-iii-28b1721fc442\">part III</a>, we will be implementing the f-beta score for multi-label classification problems. See all codes in my <a href=\"https://github.com/Jolomi-Tosanwumi/F-beta-Score-in-Keras\">GitHub repository</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=15f91f07c9a4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">F-beta Score in Keras Part II</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["f1-score-keras","f1-score","fbeta-score","keras"]},{"title":"F-beta Score in Keras Part I","pubDate":"2020-11-30 20:25:39","link":"https://towardsdatascience.com/f-beta-score-in-keras-part-i-86ad190a252f?source=rss-9d28fafd6645------2","guid":"https://medium.com/p/86ad190a252f","author":"Jolomi Tosanwumi","thumbnail":"","description":"\n<h4>Creating custom F1 score for binary classification problems in\u00a0Keras</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*QyX1vNyXnQvFEfQ9ZahBKg.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@wwarby?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">William Warby</a> on\u00a0<a href=\"https://unsplash.com/s/photos/measurement?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>During the training and evaluation of machine learning classifiers, we want to reduce type I and type II errors as much as we can. Especially when training deep learning models, we may want to monitor some metrics of interest and one of such is the F1 score (a special case of F-beta score). Unfortunately, F-beta metrics was removed in <a href=\"https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes\">Keras 2.0</a> because it can be misleading when computed in batches rather than globally (for the whole dataset). It will be more misleading if the batch size is small or when a minority class has a very small number of observations. Sometimes, many data scientists are interested in knowing the F-beta score per batch for different reasons when the batch size is\u00a0large.</p>\n<p>In this article, I will be sharing with you how to implement a custom F-beta score metric both globally (stateful) and batch-wise(stateless) in Keras. Specifically, we will deal with F-beta metric for binary classification problems in this article (part I), multi-class and multi-label classification problems in <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">part II</a> and <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-iii-28b1721fc442\">part III</a> respectively. We will assume you are familiar with the basics of deep learning, machine learning classifiers, and calculus. Before creating our custom F-beta score metric in Keras, we will look at how it is derived because sometimes, reinventing the wheel deepens one\u2019s understanding of the\u00a0wheel.</p>\n<h3>What is wrong with accuracy?</h3>\n<p>A popular metric in classification problems is the accuracy which is simply the fraction of correct predictions. The problem with this metric is that it can be misleading when using a model that is not robust to class imbalance. For example, if we have a naive model that only predict the majority class for a data that has 80% majority class and 20% minority class; the model will have an accuracy of 80% which is misleading because the model is simply just predicting only the majority class and haven\u2019t really learnt how to classify the data into its classes. We, therefore, need another metric(s) to properly evaluate such kind of\u00a0model.</p>\n<p>A binary classifier that classifies observations into positive and negative classes can have its predictions fall under one of the following four categories:</p>\n<ol>\n<li>True Positive (TP): the number of positive classes that were correctly classified.</li>\n<li>True Negative (TN): the number of negative classes that were correctly classified.</li>\n<li>False Positive (FP): the number of negative classes that were wrongly classified</li>\n<li>False Negative (FN): the number of positive class that were wrongly classified.</li>\n</ol>\n<p>Categories 1 and 2 are correct predictions, while 3 and 4 are incorrect predictions. A model\u2019s prediction under categories 3 and 4 are called type I and type II errors respectively. These four categories for better understanding can be represented in a matrix called the confusion matrix and it is as shown\u00a0below:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/145/1*HUhHrEvCm0n51R5QMg0k8Q.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>If the class of interest is the positive class, we will now introduce two metrics namely Precision and\u00a0Recall.</p>\n<ul><li>Precision is the ratio of the number of true positives to the total number of predicted positives as shown by the red rounded-rectangle in the confusion matrix above. It is the fraction of predicted positives that were correctly classified. It measures how <em>precise </em>a model is when it classifies an observation as being positive. It also tell us how well our model reduces type I error (False positive).</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/183/1*sR_w-RirBFyCN5KxAMJkEw.gif\"></figure><ul><li>Recall is the ratio of the number of true positives to the total number of actual positives as shown by the blue rounded-rectangle in the confusion matrix above. It is the fraction of actual positives that were correctly classified. It measures how well a model <em>recalls </em>the actual positive classes. It also tells us how well our model reduces type II error (False negatives). It is also called sensitivity because it measures how <em>sensitive </em>a model is to the positive\u00a0class.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/157/1*ElKHeI-lBvZMEqwUyHxMWw.gif\"></figure><p>For example, consider a model with the confusion matrix\u00a0below;</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/148/1*pQFkKiTXH9iIUgmGr2oXnA.png\"><figcaption>Image by\u00a0author</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/305/1*3gG8L-4-1kjZQEgsWuhSbw.gif\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/229/1*jERrKM29vo0794a1oOxS4A.gif\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/174/1*sl4Za795qBJOBCqFdnPENg.gif\"></figure><p>We see that although the accuracy is high, the precision is low. This makes it important to not only monitor accuracy but also monitor the precision and recall to better tell of a model\u2019s performance on an imbalance dataset.</p>\n<h3>Precision vs\u00a0Recall</h3>\n<p>The big question now is which of precision and recall should we consider as our evaluating metric? Well, it depends on our choice and the context of our problem. Precision will be our metric of interest if False Positive is more consequential than False Negative i.e. we want to avoid type I error more than type II error. For example, for a model diagnosing someone of a deadly disease, recall is more important than precision because diagnosing someone of being negative to the disease whereas the person is actually positive is highly consequential. While for a model detecting the presence of oil in a land, precision is more important than recall because predicting that oil is present whereas it isn\u2019t will make an oil drilling company incur loss due to wasted money, time, energy and resources in drilling. In this case, type I error is to be more avoided than type II\u00a0error.</p>\n<p>What if we are interested in both precision and recall that is, we want to avoid False Positives as well as False Negatives? In this case, we need a balanced tradeoff between precision and recall. This is where the f1 score comes in. The f1 score is the harmonic mean of precision and\u00a0recall.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/262/1*QPLL3KLo7w4VuoehM0HcDA.gif\"></figure><p>If you are inquisitive like me, you may want to ask why the harmonic mean? Well, harmonic mean penalizes lower values more than higher values when compared to arithmetic and geometric mean. The arithmetic, geometric and harmonic mean of 30 and 90 are 60, 51.96 and 45 respectively. Since we want to minimize type I and type II errors, we use a mean that penalizes misclassification more than correct classification hence, the harmonic mean. We don\u2019t want a model to have a high score when one of precision or recall is\u00a0low.</p>\n<p>A generalization of the f1 score is the f-beta score. The f-beta score is the weighted harmonic mean of precision and recall and it is given\u00a0by:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/212/1*bv0YmFH9qiom0knkARUzkA.gif\"></figure><p>Where P is Precision, R is the Recall, <em>\u03b1 </em>is the weight we give to Precision while (1-<em>\u03b1</em>) is the weight we give to Recall. Notice that the sum of the weights of Precision and Recall is 1. Making f-beta the subject of the formula, we\u00a0have:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/172/1*zQ0XT4QhM2IQK4synWC71Q.gif\"></figure><p>We cannot talk about f-beta score without mentioning C. J. Van Rijsbergen. In chapter 7 of his book[1]<em>, </em>he laid the premise on which the f-beta score is now being calculated.</p>\n<blockquote>Finally, we incorporate into our measurement procedure the fact that users may attach different relative importance to precision and recall. What we want is therefore a parameter (<em>\u03b2</em>) to characterize the measurement function in such a way that we can say: it measures the effectiveness of retrieval with respect to a user who attaches \u00df times as much importance to recall as precision. The simplest way I know of quantifying this is to specify the <em>P/R</em> ratio at which the user is willing to trade an increment in precision for an equal loss in\u00a0recall.</blockquote>\n<blockquote>\u2014 <strong>C. J. Van Rijsbergen</strong>.</blockquote>\n<p>In his masterpiece, Van Rijsbergen went on to define this relative importance as the P/R ratio at\u00a0which:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/77/1*aCYLjHJalCuv7aN_d2CVGw.gif\"></figure><p>where <em>E </em>is the measure of effectiveness based on precision and\u00a0recall.</p>\n<p>Van Rijsbergen used Effectiveness instead of F-beta. Effectiveness is actually (1- f-beta) therefore we can also define the relative importance as the P/R ratio at\u00a0which:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/84/1*m2_NvGm0zoNd3vxvGzq8eA.gif\"></figure><p>Applying the differential equation above to the f-beta formula by taking the partial differential of f-beta with respect to recall and equating it to the partial differential of f-beta with respect to precision; the resulting equation can be reduced\u00a0to:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/105/1*dYxayQ2Mb36lNOHbpqGRtg.gif\"></figure><p>Although, Van Rijsbergen used P/R ratio, <em>\u03b2 </em>is actually defined as the R/P ratio. Therefore:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/142/1*BRbmxTnT5yMIGPw2qNBgFg.gif\"></figure><p>This implies\u00a0that:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/87/1*CfH8IYINwRdMDPb-231J4w.gif\"></figure><p>Therefore, beta-squared is the ratio of the weight of Recall to the weight of Precision. F-beta formula finally\u00a0becomes:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/143/1*c_2qtH0h-vVGGcG4qPzDiQ.gif\"></figure><p>We now see that f1 score is a special case of f-beta where beta = 1. Also, we can have f.5, f2 scores e.t.c. depending on how much weight a user gives to recall. A little consideration will show that if beta is greater than 1, recall is weighted more than precision, while precision is weighted more than recall if beta is lesser than 1. If better equals 1, we have no preference for recall or precision but penalize the lower of\u00a0them.</p>\n<h3>Stateless F-beta</h3>\n<p>Stateless metric according to <a href=\"https://keras.io/api/metrics/#as-simple-callables-stateless\">Keras documentation</a> means that the metric is estimated per batch. Therefore, the last metric reported after training is actually that of the last batch. Sometimes, we may want to monitor a metric per batch during training especially when the batch size is large, validation data size is the expected test size or due to the fact that weights of nodes are updated per batch. To demonstrate how to implement this in Keras, we will be using the famous Modified National Institute of Standards and Technology (MNIST) <a href=\"https://en.wikipedia.org/wiki/MNIST_database#:~:text=The%20MNIST%20database%20(%20National%20Institute,the%20field%20of%20machine%20learning.\">dataset </a>which is a dataset of 60,000 training and 10,000 testing 28x28 grayscale images of handwritten digits between 0 and 9 (inclusive). Since we are focusing on binary classification in this article, we will tweak our task to a binary classification problem of predicting if an image is that of an even number or an odd number. Now, let\u2019s start coding. First, we import useful libraries for our\u00a0task.</p>\n<a href=\"https://medium.com/media/79e560831d573504b8025aac9c72e445/href\">https://medium.com/media/79e560831d573504b8025aac9c72e445/href</a><p>Let\u2019s load in the\u00a0dataset.</p>\n<a href=\"https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href\">https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*s0fPhKuPijZ0BcvpCtVjQA.png\"></figure><p>Let\u2019s randomly view some of the images and their corresponding labels.</p>\n<a href=\"https://medium.com/media/281ee17ebef35194d55f018d81a62568/href\">https://medium.com/media/281ee17ebef35194d55f018d81a62568/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/344/1*t50SUMwXCxIY0QN-EgBMTQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>Next, we rescale the images, converts the labels to binary (1 for even numbers and 0 for odd numbers).</p>\n<a href=\"https://medium.com/media/4d15da711d7c2762da37e719d67b79b3/href\">https://medium.com/media/4d15da711d7c2762da37e719d67b79b3/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/347/1*xtC2umQAPknkuCbKdOeVAQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>We will now show the first way we can calculate the f1 score during training by using that of Scikit-learn. When using Keras with Tensorflow, functions not wrapped in <em>tf.function</em> logic can only be used when eager execution is disabled hence, we will call our f-beta function <em>eager_binary_fbeta</em>. Remember we have imported fbeta_score from Scikit-learn.</p>\n<a href=\"https://medium.com/media/bbb7eece2f1ee01da257748dd6535d0b/href\">https://medium.com/media/bbb7eece2f1ee01da257748dd6535d0b/href</a><p>We will now define a function to build our model. The aim of this article is to demonstrate how to create custom f-beta score metric and not to build a high performance model. So, we will build a simple convolutional neural network which will run for few epochs. We will also set <em>run_eagerly </em>to <em>True </em>because we want to use Scikit-learn\u2019s f-beta score metric during training.</p>\n<a href=\"https://medium.com/media/2d22b548a01ff4ec19a946da60b68c51/href\">https://medium.com/media/2d22b548a01ff4ec19a946da60b68c51/href</a><a href=\"https://medium.com/media/fae4e523003533fab12da5186a1c2dbe/href\">https://medium.com/media/fae4e523003533fab12da5186a1c2dbe/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Yu19c-ZZqGCTG84sskpHJA.png\"></figure><p>This worked out nicely but with one problem; the elapsed time per epoch. Since we ran the model eagerly, we expect a high time complexity which will worsen when working with more complex neural networks, larger datasets or smaller batch size. We will now see how to create a custom f-beta score metric which would be wrapped in <em>tf.function</em> logics and wouldn\u2019t be run eagerly. We will simply call this function <em>binary_fbeta. </em>Implementation of this function will be possible based on the facts that for <em>ytrue </em>and <em>ypred </em>arrays of a binary classification problem where 1 is the positive class and 0 is the negative\u00a0class:</p>\n<ol>\n<li>True positive is the sum of the element-wise multiplication of the two\u00a0arrays.</li>\n<li>Predicted positive is the sum of\u00a0<em>ypred.</em>\n</li>\n<li>Actual positive is the sum of\u00a0<em>ytrue.</em>\n</li>\n</ol>\n<a href=\"https://medium.com/media/18b7e4332cb324cdb8fcd5e3f3a7c764/href\">https://medium.com/media/18b7e4332cb324cdb8fcd5e3f3a7c764/href</a><a href=\"https://medium.com/media/abe63f74aad1785c43505e019d589b67/href\">https://medium.com/media/abe63f74aad1785c43505e019d589b67/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3y_c38j9enTAsOUA5t9t-A.png\"></figure><p>We now see about 22% decrease in the elapsed time per epoch. I will advice using this method for speed. You can ignore the warnings for now. The slight changes in the reported metrics compared to the first method is because of some randomized processes we didn\u2019t seed. Although we seeded some(which reduced the differences), there are still other randomizes processes especially when using a GPU. Let\u2019s confirm the rightness of our custom f-beta function by comparing its evaluation of the testing set to that of Scikit-learn\u2019s f-beta function.</p>\n<a href=\"https://medium.com/media/857c0b141f8e7493f4d776c2689464de/href\">https://medium.com/media/857c0b141f8e7493f4d776c2689464de/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/576/1*GRyPtPnzsl_iZKTQTxqX9w.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/345/1*krc0cvttq4XL4FTz_yPwUQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>From the f1_scores printed out, we can conclude that our custom f-beta is working as expected. Our model also did a pretty good job in recognizing even and odd numbers as shown in the image\u00a0above.</p>\n<h3>Stateful F-beta</h3>\n<p>When we are not interested in the per batch metric but in the metric evaluated on the whole dataset, we need to subclass the <em>Metric </em>class so that a state is maintained across all batches. This maintained state is made possible by keeping track of variables (called state variables) that are useful in evaluating our metric across all batches. When this happens, our metric is said to be stateful. The last metric reported after training is actually that of the whole dataset (you could set verbose to 2 in the model\u2019s fit method so as to report only the metric of the last batch which is that of the whole dataset for stateful metrics). According to <a href=\"https://keras.io/api/metrics/#as-subclasses-of-metric-stateful\">Keras documentation</a>, there are four <em>methods </em>a stateful metric should\u00a0have:</p>\n<ol>\n<li>__init__\u00a0: we create (initialize) the state variables here.</li>\n<li>update: this method is called at the end of each batch and is used to change (update) the state variables.</li>\n<li>result: this is called at the end of each batch after states variables are updated. It is used to compute and return the metric for each\u00a0batch.</li>\n<li>reset: this is called at the end of each epoch. It is used to clear (reinitialize) the state variables.</li>\n</ol>\n<p>For binary f-beta, state variables would definitely be true positives, actual positives and predicted positives because they can easily be tracked across all batches. Let\u2019s now implement a stateful f-beta metric for our binary classification problem.</p>\n<a href=\"https://medium.com/media/a1de237870d7770b41d7bbc0951e8e4c/href\">https://medium.com/media/a1de237870d7770b41d7bbc0951e8e4c/href</a><a href=\"https://medium.com/media/7c9dadc80bdf152dfbf9a696f157a256/href\">https://medium.com/media/7c9dadc80bdf152dfbf9a696f157a256/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sNkGxCGO48AbG-Hn6ktn0g.png\"></figure><p>Finally, we will check the rightness of our stateful f-beta by comparing it with Scikit-learn\u2019s f-beta score metric on some randomly generated arrays of ones and\u00a0zeros.</p>\n<a href=\"https://medium.com/media/eedfff9fb252d9279363eaea9ac9304a/href\">https://medium.com/media/eedfff9fb252d9279363eaea9ac9304a/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/548/1*NUMjlI-p-c4iMlK5fMQRsw.png\"></figure><h3>Conclusion</h3>\n<p>F-beta score can be implemented in Keras for binary classification either as a stateful or a stateless metric as we have seen in this article. We have also seen how to derive the formula for f-beta score. In <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">part II</a>, we will be implementing the f-beta score for multiclass problems. See all codes in my <a href=\"https://github.com/Jolomi-Tosanwumi/F-beta-Score-in-Keras\">GitHub repository</a>.</p>\n<h3>References</h3>\n<p>[1]<em> </em>C. J. Van Rijsbergen,<em> </em><a href=\"http://www.dcs.gla.ac.uk/Keith/Preface.html\"><em>Information Retrieval</em></a><em> </em>(1979).</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=86ad190a252f\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-i-86ad190a252f\">F-beta Score in Keras Part I</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Creating custom F1 score for binary classification problems in\u00a0Keras</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*QyX1vNyXnQvFEfQ9ZahBKg.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@wwarby?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">William Warby</a> on\u00a0<a href=\"https://unsplash.com/s/photos/measurement?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>During the training and evaluation of machine learning classifiers, we want to reduce type I and type II errors as much as we can. Especially when training deep learning models, we may want to monitor some metrics of interest and one of such is the F1 score (a special case of F-beta score). Unfortunately, F-beta metrics was removed in <a href=\"https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes\">Keras 2.0</a> because it can be misleading when computed in batches rather than globally (for the whole dataset). It will be more misleading if the batch size is small or when a minority class has a very small number of observations. Sometimes, many data scientists are interested in knowing the F-beta score per batch for different reasons when the batch size is\u00a0large.</p>\n<p>In this article, I will be sharing with you how to implement a custom F-beta score metric both globally (stateful) and batch-wise(stateless) in Keras. Specifically, we will deal with F-beta metric for binary classification problems in this article (part I), multi-class and multi-label classification problems in <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">part II</a> and <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-iii-28b1721fc442\">part III</a> respectively. We will assume you are familiar with the basics of deep learning, machine learning classifiers, and calculus. Before creating our custom F-beta score metric in Keras, we will look at how it is derived because sometimes, reinventing the wheel deepens one\u2019s understanding of the\u00a0wheel.</p>\n<h3>What is wrong with accuracy?</h3>\n<p>A popular metric in classification problems is the accuracy which is simply the fraction of correct predictions. The problem with this metric is that it can be misleading when using a model that is not robust to class imbalance. For example, if we have a naive model that only predict the majority class for a data that has 80% majority class and 20% minority class; the model will have an accuracy of 80% which is misleading because the model is simply just predicting only the majority class and haven\u2019t really learnt how to classify the data into its classes. We, therefore, need another metric(s) to properly evaluate such kind of\u00a0model.</p>\n<p>A binary classifier that classifies observations into positive and negative classes can have its predictions fall under one of the following four categories:</p>\n<ol>\n<li>True Positive (TP): the number of positive classes that were correctly classified.</li>\n<li>True Negative (TN): the number of negative classes that were correctly classified.</li>\n<li>False Positive (FP): the number of negative classes that were wrongly classified</li>\n<li>False Negative (FN): the number of positive class that were wrongly classified.</li>\n</ol>\n<p>Categories 1 and 2 are correct predictions, while 3 and 4 are incorrect predictions. A model\u2019s prediction under categories 3 and 4 are called type I and type II errors respectively. These four categories for better understanding can be represented in a matrix called the confusion matrix and it is as shown\u00a0below:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/145/1*HUhHrEvCm0n51R5QMg0k8Q.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>If the class of interest is the positive class, we will now introduce two metrics namely Precision and\u00a0Recall.</p>\n<ul><li>Precision is the ratio of the number of true positives to the total number of predicted positives as shown by the red rounded-rectangle in the confusion matrix above. It is the fraction of predicted positives that were correctly classified. It measures how <em>precise </em>a model is when it classifies an observation as being positive. It also tell us how well our model reduces type I error (False positive).</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/183/1*sR_w-RirBFyCN5KxAMJkEw.gif\"></figure><ul><li>Recall is the ratio of the number of true positives to the total number of actual positives as shown by the blue rounded-rectangle in the confusion matrix above. It is the fraction of actual positives that were correctly classified. It measures how well a model <em>recalls </em>the actual positive classes. It also tells us how well our model reduces type II error (False negatives). It is also called sensitivity because it measures how <em>sensitive </em>a model is to the positive\u00a0class.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/157/1*ElKHeI-lBvZMEqwUyHxMWw.gif\"></figure><p>For example, consider a model with the confusion matrix\u00a0below;</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/148/1*pQFkKiTXH9iIUgmGr2oXnA.png\"><figcaption>Image by\u00a0author</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/305/1*3gG8L-4-1kjZQEgsWuhSbw.gif\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/229/1*jERrKM29vo0794a1oOxS4A.gif\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/174/1*sl4Za795qBJOBCqFdnPENg.gif\"></figure><p>We see that although the accuracy is high, the precision is low. This makes it important to not only monitor accuracy but also monitor the precision and recall to better tell of a model\u2019s performance on an imbalance dataset.</p>\n<h3>Precision vs\u00a0Recall</h3>\n<p>The big question now is which of precision and recall should we consider as our evaluating metric? Well, it depends on our choice and the context of our problem. Precision will be our metric of interest if False Positive is more consequential than False Negative i.e. we want to avoid type I error more than type II error. For example, for a model diagnosing someone of a deadly disease, recall is more important than precision because diagnosing someone of being negative to the disease whereas the person is actually positive is highly consequential. While for a model detecting the presence of oil in a land, precision is more important than recall because predicting that oil is present whereas it isn\u2019t will make an oil drilling company incur loss due to wasted money, time, energy and resources in drilling. In this case, type I error is to be more avoided than type II\u00a0error.</p>\n<p>What if we are interested in both precision and recall that is, we want to avoid False Positives as well as False Negatives? In this case, we need a balanced tradeoff between precision and recall. This is where the f1 score comes in. The f1 score is the harmonic mean of precision and\u00a0recall.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/262/1*QPLL3KLo7w4VuoehM0HcDA.gif\"></figure><p>If you are inquisitive like me, you may want to ask why the harmonic mean? Well, harmonic mean penalizes lower values more than higher values when compared to arithmetic and geometric mean. The arithmetic, geometric and harmonic mean of 30 and 90 are 60, 51.96 and 45 respectively. Since we want to minimize type I and type II errors, we use a mean that penalizes misclassification more than correct classification hence, the harmonic mean. We don\u2019t want a model to have a high score when one of precision or recall is\u00a0low.</p>\n<p>A generalization of the f1 score is the f-beta score. The f-beta score is the weighted harmonic mean of precision and recall and it is given\u00a0by:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/212/1*bv0YmFH9qiom0knkARUzkA.gif\"></figure><p>Where P is Precision, R is the Recall, <em>\u03b1 </em>is the weight we give to Precision while (1-<em>\u03b1</em>) is the weight we give to Recall. Notice that the sum of the weights of Precision and Recall is 1. Making f-beta the subject of the formula, we\u00a0have:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/172/1*zQ0XT4QhM2IQK4synWC71Q.gif\"></figure><p>We cannot talk about f-beta score without mentioning C. J. Van Rijsbergen. In chapter 7 of his book[1]<em>, </em>he laid the premise on which the f-beta score is now being calculated.</p>\n<blockquote>Finally, we incorporate into our measurement procedure the fact that users may attach different relative importance to precision and recall. What we want is therefore a parameter (<em>\u03b2</em>) to characterize the measurement function in such a way that we can say: it measures the effectiveness of retrieval with respect to a user who attaches \u00df times as much importance to recall as precision. The simplest way I know of quantifying this is to specify the <em>P/R</em> ratio at which the user is willing to trade an increment in precision for an equal loss in\u00a0recall.</blockquote>\n<blockquote>\u2014 <strong>C. J. Van Rijsbergen</strong>.</blockquote>\n<p>In his masterpiece, Van Rijsbergen went on to define this relative importance as the P/R ratio at\u00a0which:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/77/1*aCYLjHJalCuv7aN_d2CVGw.gif\"></figure><p>where <em>E </em>is the measure of effectiveness based on precision and\u00a0recall.</p>\n<p>Van Rijsbergen used Effectiveness instead of F-beta. Effectiveness is actually (1- f-beta) therefore we can also define the relative importance as the P/R ratio at\u00a0which:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/84/1*m2_NvGm0zoNd3vxvGzq8eA.gif\"></figure><p>Applying the differential equation above to the f-beta formula by taking the partial differential of f-beta with respect to recall and equating it to the partial differential of f-beta with respect to precision; the resulting equation can be reduced\u00a0to:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/105/1*dYxayQ2Mb36lNOHbpqGRtg.gif\"></figure><p>Although, Van Rijsbergen used P/R ratio, <em>\u03b2 </em>is actually defined as the R/P ratio. Therefore:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/142/1*BRbmxTnT5yMIGPw2qNBgFg.gif\"></figure><p>This implies\u00a0that:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/87/1*CfH8IYINwRdMDPb-231J4w.gif\"></figure><p>Therefore, beta-squared is the ratio of the weight of Recall to the weight of Precision. F-beta formula finally\u00a0becomes:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/143/1*c_2qtH0h-vVGGcG4qPzDiQ.gif\"></figure><p>We now see that f1 score is a special case of f-beta where beta = 1. Also, we can have f.5, f2 scores e.t.c. depending on how much weight a user gives to recall. A little consideration will show that if beta is greater than 1, recall is weighted more than precision, while precision is weighted more than recall if beta is lesser than 1. If better equals 1, we have no preference for recall or precision but penalize the lower of\u00a0them.</p>\n<h3>Stateless F-beta</h3>\n<p>Stateless metric according to <a href=\"https://keras.io/api/metrics/#as-simple-callables-stateless\">Keras documentation</a> means that the metric is estimated per batch. Therefore, the last metric reported after training is actually that of the last batch. Sometimes, we may want to monitor a metric per batch during training especially when the batch size is large, validation data size is the expected test size or due to the fact that weights of nodes are updated per batch. To demonstrate how to implement this in Keras, we will be using the famous Modified National Institute of Standards and Technology (MNIST) <a href=\"https://en.wikipedia.org/wiki/MNIST_database#:~:text=The%20MNIST%20database%20(%20National%20Institute,the%20field%20of%20machine%20learning.\">dataset </a>which is a dataset of 60,000 training and 10,000 testing 28x28 grayscale images of handwritten digits between 0 and 9 (inclusive). Since we are focusing on binary classification in this article, we will tweak our task to a binary classification problem of predicting if an image is that of an even number or an odd number. Now, let\u2019s start coding. First, we import useful libraries for our\u00a0task.</p>\n<a href=\"https://medium.com/media/79e560831d573504b8025aac9c72e445/href\">https://medium.com/media/79e560831d573504b8025aac9c72e445/href</a><p>Let\u2019s load in the\u00a0dataset.</p>\n<a href=\"https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href\">https://medium.com/media/13008ca4144ae731c6323f75454f4aa2/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*s0fPhKuPijZ0BcvpCtVjQA.png\"></figure><p>Let\u2019s randomly view some of the images and their corresponding labels.</p>\n<a href=\"https://medium.com/media/281ee17ebef35194d55f018d81a62568/href\">https://medium.com/media/281ee17ebef35194d55f018d81a62568/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/344/1*t50SUMwXCxIY0QN-EgBMTQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>Next, we rescale the images, converts the labels to binary (1 for even numbers and 0 for odd numbers).</p>\n<a href=\"https://medium.com/media/4d15da711d7c2762da37e719d67b79b3/href\">https://medium.com/media/4d15da711d7c2762da37e719d67b79b3/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/347/1*xtC2umQAPknkuCbKdOeVAQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>We will now show the first way we can calculate the f1 score during training by using that of Scikit-learn. When using Keras with Tensorflow, functions not wrapped in <em>tf.function</em> logic can only be used when eager execution is disabled hence, we will call our f-beta function <em>eager_binary_fbeta</em>. Remember we have imported fbeta_score from Scikit-learn.</p>\n<a href=\"https://medium.com/media/bbb7eece2f1ee01da257748dd6535d0b/href\">https://medium.com/media/bbb7eece2f1ee01da257748dd6535d0b/href</a><p>We will now define a function to build our model. The aim of this article is to demonstrate how to create custom f-beta score metric and not to build a high performance model. So, we will build a simple convolutional neural network which will run for few epochs. We will also set <em>run_eagerly </em>to <em>True </em>because we want to use Scikit-learn\u2019s f-beta score metric during training.</p>\n<a href=\"https://medium.com/media/2d22b548a01ff4ec19a946da60b68c51/href\">https://medium.com/media/2d22b548a01ff4ec19a946da60b68c51/href</a><a href=\"https://medium.com/media/fae4e523003533fab12da5186a1c2dbe/href\">https://medium.com/media/fae4e523003533fab12da5186a1c2dbe/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Yu19c-ZZqGCTG84sskpHJA.png\"></figure><p>This worked out nicely but with one problem; the elapsed time per epoch. Since we ran the model eagerly, we expect a high time complexity which will worsen when working with more complex neural networks, larger datasets or smaller batch size. We will now see how to create a custom f-beta score metric which would be wrapped in <em>tf.function</em> logics and wouldn\u2019t be run eagerly. We will simply call this function <em>binary_fbeta. </em>Implementation of this function will be possible based on the facts that for <em>ytrue </em>and <em>ypred </em>arrays of a binary classification problem where 1 is the positive class and 0 is the negative\u00a0class:</p>\n<ol>\n<li>True positive is the sum of the element-wise multiplication of the two\u00a0arrays.</li>\n<li>Predicted positive is the sum of\u00a0<em>ypred.</em>\n</li>\n<li>Actual positive is the sum of\u00a0<em>ytrue.</em>\n</li>\n</ol>\n<a href=\"https://medium.com/media/18b7e4332cb324cdb8fcd5e3f3a7c764/href\">https://medium.com/media/18b7e4332cb324cdb8fcd5e3f3a7c764/href</a><a href=\"https://medium.com/media/abe63f74aad1785c43505e019d589b67/href\">https://medium.com/media/abe63f74aad1785c43505e019d589b67/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3y_c38j9enTAsOUA5t9t-A.png\"></figure><p>We now see about 22% decrease in the elapsed time per epoch. I will advice using this method for speed. You can ignore the warnings for now. The slight changes in the reported metrics compared to the first method is because of some randomized processes we didn\u2019t seed. Although we seeded some(which reduced the differences), there are still other randomizes processes especially when using a GPU. Let\u2019s confirm the rightness of our custom f-beta function by comparing its evaluation of the testing set to that of Scikit-learn\u2019s f-beta function.</p>\n<a href=\"https://medium.com/media/857c0b141f8e7493f4d776c2689464de/href\">https://medium.com/media/857c0b141f8e7493f4d776c2689464de/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/576/1*GRyPtPnzsl_iZKTQTxqX9w.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/345/1*krc0cvttq4XL4FTz_yPwUQ.png\"><figcaption>Image by\u00a0author</figcaption></figure><p>From the f1_scores printed out, we can conclude that our custom f-beta is working as expected. Our model also did a pretty good job in recognizing even and odd numbers as shown in the image\u00a0above.</p>\n<h3>Stateful F-beta</h3>\n<p>When we are not interested in the per batch metric but in the metric evaluated on the whole dataset, we need to subclass the <em>Metric </em>class so that a state is maintained across all batches. This maintained state is made possible by keeping track of variables (called state variables) that are useful in evaluating our metric across all batches. When this happens, our metric is said to be stateful. The last metric reported after training is actually that of the whole dataset (you could set verbose to 2 in the model\u2019s fit method so as to report only the metric of the last batch which is that of the whole dataset for stateful metrics). According to <a href=\"https://keras.io/api/metrics/#as-subclasses-of-metric-stateful\">Keras documentation</a>, there are four <em>methods </em>a stateful metric should\u00a0have:</p>\n<ol>\n<li>__init__\u00a0: we create (initialize) the state variables here.</li>\n<li>update: this method is called at the end of each batch and is used to change (update) the state variables.</li>\n<li>result: this is called at the end of each batch after states variables are updated. It is used to compute and return the metric for each\u00a0batch.</li>\n<li>reset: this is called at the end of each epoch. It is used to clear (reinitialize) the state variables.</li>\n</ol>\n<p>For binary f-beta, state variables would definitely be true positives, actual positives and predicted positives because they can easily be tracked across all batches. Let\u2019s now implement a stateful f-beta metric for our binary classification problem.</p>\n<a href=\"https://medium.com/media/a1de237870d7770b41d7bbc0951e8e4c/href\">https://medium.com/media/a1de237870d7770b41d7bbc0951e8e4c/href</a><a href=\"https://medium.com/media/7c9dadc80bdf152dfbf9a696f157a256/href\">https://medium.com/media/7c9dadc80bdf152dfbf9a696f157a256/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sNkGxCGO48AbG-Hn6ktn0g.png\"></figure><p>Finally, we will check the rightness of our stateful f-beta by comparing it with Scikit-learn\u2019s f-beta score metric on some randomly generated arrays of ones and\u00a0zeros.</p>\n<a href=\"https://medium.com/media/eedfff9fb252d9279363eaea9ac9304a/href\">https://medium.com/media/eedfff9fb252d9279363eaea9ac9304a/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/548/1*NUMjlI-p-c4iMlK5fMQRsw.png\"></figure><h3>Conclusion</h3>\n<p>F-beta score can be implemented in Keras for binary classification either as a stateful or a stateless metric as we have seen in this article. We have also seen how to derive the formula for f-beta score. In <a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-ii-15f91f07c9a4\">part II</a>, we will be implementing the f-beta score for multiclass problems. See all codes in my <a href=\"https://github.com/Jolomi-Tosanwumi/F-beta-Score-in-Keras\">GitHub repository</a>.</p>\n<h3>References</h3>\n<p>[1]<em> </em>C. J. Van Rijsbergen,<em> </em><a href=\"http://www.dcs.gla.ac.uk/Keith/Preface.html\"><em>Information Retrieval</em></a><em> </em>(1979).</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=86ad190a252f\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/f-beta-score-in-keras-part-i-86ad190a252f\">F-beta Score in Keras Part I</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["f1-score-in-keras","f1-score","keras","fbeta-score","editors-pick"]},{"title":"Using Vector Auto-Regression to Model Bitcoin Prices","pubDate":"2020-10-30 11:00:05","link":"https://medium.com/hamoye-blogs/using-vector-auto-regression-to-model-bitcoin-prices-6cae0327ac30?source=rss-9d28fafd6645------2","guid":"https://medium.com/p/6cae0327ac30","author":"Jolomi Tosanwumi","thumbnail":"","description":"\n<h4>Implementing multivariate time series in\u00a0Python</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SN3tfnqnqf_Dc-2wY13-8A.jpeg\"><figcaption>Photo from Unsplash by Andre Francois\u00a0McKenzie</figcaption></figure><p>As our stage E project in the <a href=\"https://hamoye.com/\">Hamoye</a> data science internship, my group forecasted Bitcoin\u2019s prices. In this article, I will be sharing the approach I used in forecasting Bitcoin\u2019s Open, High, Low, Close (OHLC), and Weighed price. The <a href=\"https://www.kaggle.com/mczielinski/bitcoin-historical-data\">dataset</a> is a minute sampling of Bitcoin\u2019s historical data from 31st December 2011 to 14th September 2020 available on\u00a0Kaggle.</p>\n<p>Vector Auto-Regression (VAR) is the multivariate version of Auto-Regression (AR). Unlike the AR model which represents the future value of a time series as a function of its past values (lag values of itself), the VAR model represents the future value of a time series as a function of its past values and those of other related time series. It is more advantageous as it considers the interdependencies amongst different time\u00a0series.</p>\n<p>In this article, we will be using VAR to model Bitcoin\u2019s OHLC and Weighted price. We will assume you are familiar with the basics of time series, <a href=\"https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\">Augmented Dickey-Fuller (ADF) test</a>, <a href=\"https://en.wikipedia.org/wiki/Johansen_test\">Johansen\u2019s co-integration test</a>, and <a href=\"https://en.wikipedia.org/wiki/Granger_causality\">Granger\u2019s causality test</a>.</p>\n<h3>Loading in the\u00a0Data</h3>\n<pre><em># importing useful libraries<br></em>import numpy as np<br>import pandas as pd<br>import matplotlib.pyplot as plt<br>import seaborn as sns </pre>\n<pre>import warnings<br>warnings.filterwarnings('ignore') </pre>\n<pre>%matplotlib inline</pre>\n<pre><em># loading in the data<br></em>df = pd.read_csv('../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv')<br>print(df.shape) <em># prints \"(4572257, 8)\"</em></pre>\n<pre>bitcoin_df = df.copy() <em># making a copy of the dataset to work with</em><br>df.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/582/1*Hr8_DXxqhhxJbvXNslWOaw.png\"></figure><h3>Data Visualization</h3>\n<a href=\"https://medium.com/media/0bc7fee8c59c233cc9c1688f292d3dfd/href\">https://medium.com/media/0bc7fee8c59c233cc9c1688f292d3dfd/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1ET_TLEJKlkAPlDNQVrHdg.png\"></figure><h3>Preprocessing</h3>\n<pre><em># converting the 'Timestamp' column to datetime object<br></em>time_col = pd.to_datetime(bitcoin_df['Timestamp'], unit='s') </pre>\n<pre><em># dropping the 'Timestamp' column <br></em>bitcoin_df.drop('Timestamp', axis=1, inplace=True) </pre>\n<pre><em># creating a new 'Timestamp' column with datetime dtype<br></em>bitcoin_df['Timestamp'] = time_col  </pre>\n<pre><em># makes 'Timestamp' the index<br></em>bitcoin_df.set_index('Timestamp', inplace=True) </pre>\n<pre><em># creating a dataframe with boolean values indicating the presence<br># of missing values<br></em>missing_val_mask = bitcoin_df.isnull()<br>missing_val_mask.sum()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/235/1*n-ybskejXgvw0wH5lRZxZQ.png\"></figure><p>Let\u2019s check if all columns have Nan values for each row with any Nan\u00a0value.</p>\n<pre>print(missing_val_mask.sum(axis=1).sum() == len(bitcoin_df.columns) * missing_val_mask.sum().loc['Open']) </pre>\n<pre><em># prints \"True\"</em></pre>\n<p>So, we have confirmed that all columns have Nan values for each row with any Nan value. Now, we check the number of rows with missing\u00a0values.</p>\n<pre>n_missing_val = missing_val_mask.sum().loc['Open'] <br>print('There are {} rows with missing values which make up {}% of the total number of rows'.format(n_missing_val, round(100*n_missing_val / bitcoin_df.shape[0], 3)))</pre>\n<pre><em># prints \"There are 1241716 rows with missing values which make up 27.158% of the total number of rows\"</em></pre>\n<p>Let\u2019s see the reason for the missing values as stated by Zielak\u200a\u2014<em>\u200athe provider of the dataset</em>. This will make us know how to handle\u00a0them.</p>\n<blockquote>\n<em>\u2026</em><strong><em>TIMESTAMPS WITHOUT ANY TRADES OR ACTIVITY HAVE THEIR DATA FIELDS FILLED WITH\u00a0NANS</em></strong><em>\u2026</em>\n</blockquote>\n<p>Zielak also told us about jumps in timestamps and here is the\u00a0reason:</p>\n<blockquote>\n<strong><em>\u2026IF A TIMESTAMP IS MISSING, OR IF THERE ARE JUMPS, THIS MAY BE BECAUSE THE EXCHANGE (OR ITS API) WAS DOWN, THE EXCHANGE (OR ITS API) DID NOT EXIST, OR SOME OTHER UNFORESEEN TECHNICAL ERROR IN THE DATA REPORTING OR GATHERING</em></strong>.</blockquote>\n<p>Since the cause of <em>missingness</em> is \u201cno trading activity\u201d, we will use any of the following two ways to handle the missing\u00a0values:</p>\n<ol>\n<li>By imputing Volume_(BTC) and Volume_(Currency)with zeros since there was no trading activity. Then, we do forward fill for Close and impute Open, High, Low and Weighted_Price with the values ofClose so the Bitcoin chart will be a horizontal line for that period of inactivity.</li>\n<li>Dropping rows with Nan values because there were no trades for those\u00a0periods.</li>\n</ol>\n<p>We will be using the first approach to build our model because we know the reason for <em>missingness </em>and the percentage of Nan values is\u00a027.158%.</p>\n<a href=\"https://medium.com/media/f6bc13a26b0f687c861a8d5baab425fd/href\">https://medium.com/media/f6bc13a26b0f687c861a8d5baab425fd/href</a><pre>bitcoin_df_imputed = impute(bitcoin_df) # imputes 'bitcoin_df'</pre>\n<pre><em># checking for Nan values after imputing<br></em>print(bitcoin_df_imputed.isnull().sum().sum()) <br><em># prints \"0\"</em></pre>\n<pre>bitcoin_df_imputed.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*EWr25nj_N_Q8OXys_FnHfQ.png\"></figure><h3>Resampling</h3>\n<p>We will be down-sampling to a daily frequency. After resampling, we will be using first, max, min, last as aggregation functions for each of OHLC respectively. We will then use sum as those of Volume(BTC) and Volume(Currency\u00a0. Weighted_Price is the ratio of Volume(Currency) to Volume(BTC)\u00a0.</p>\n<pre>daily_Open = bitcoin_df_imputed['Open'].resample('D').first()<br>daily_High = bitcoin_df_imputed['High'].resample('D').max()<br>daily_Low = bitcoin_df_imputed['Low'].resample('D').min()<br>daily_Close = bitcoin_df_imputed['Close'].resample('D').last()<br>daily_Volume_BTC = bitcoin_df_imputed['Volume_(BTC)'].resample('D').sum()<br>daily_Volume_Currency = bitcoin_df_imputed['Volume_(Currency)'].resample('D').sum()</pre>\n<pre><em># making a dataframe of the resampled time series<br></em>daily_df = pd.DataFrame({'Open': daily_Open, 'High': daily_High, 'Low': daily_Low, 'Close': daily_Close, 'Volume_(BTC)': daily_Volume_BTC, 'Volume_(Currency)': daily_Volume_Currency})</pre>\n<pre><em># checking for Nan values after resampling.<br></em>daily_df.isnull().sum().sum() <em># prints 12</em></pre>\n<p>After resampling, we see that OHLC has Nan values in them. This is because of the presence of time jumps in the data. We will impute them using the same approach as\u00a0before.</p>\n<pre>daily_df_imputed = impute(daily_df)<br>print('bitcoin_df_imputed has been downsampled from a minute timeframe of {}'.format(bitcoin_df_imputed.shape[0])\\<br>+ ' observations to a daily timeframe of {} observations'.format( \\ daily_df_imputed.shape[0]))</pre>\n<pre><em># prints \"bitcoin_df_imputed has been downsampled from a minute timeframe of 4572257 observations to a daily timeframe of 3181 observations\"</em></pre>\n<p>Let\u2019s check the number of observations in the last day of our actual\u00a0dataset.</p>\n<pre>bitcoin_df.tail()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/709/1*NfG2ju2Bsvq5zDEUbjuZ1g.png\"></figure><p>Since the actual data ended on 2020\u201309\u201314 00:00:00, we will do away with the last observation because that day has only one observation in the minute time\u00a0frame.</p>\n<pre>daily_df_imputed = daily_df_imputed.iloc[:-1].copy()<br>daily_df_imputed.tail()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/650/1*50pPcZd0cVbgHGsLBIVhrw.png\"></figure><p>Let\u2019s check the plots of the daily resampled series time\u00a0series.</p>\n<pre>plot_series_of_each_column(daily_df_imputed)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/811/1*0LsJHvNQTYnhjXRZg3Wyyg.png\"></figure><h3>Correlation Test</h3>\n<pre><em># correlation heatmap</em></pre>\n<pre><em># masking out the upper-traingular matrix<br></em>mask = np.triu(daily_df_imputed.corr())</pre>\n<pre>plt.figure(figsize=(7, 7)) <br>sns.heatmap(daily_df_imputed.corr(), mask=mask, xticklabels=True, yticklabels=True, cmap='coolwarm', annot=True)<br><em># xticklabels and yticklabels are set to True to display all columns # in the heatmap<br></em>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/490/1*J5PP0wlbPlOP0q_uj4mc1g.png\"></figure><p>From the correlation heat-map, we see that OHLC and Weighted_price are highly correlated therefore, we will group them as our multivariate time series. We could also include Volume(Currency) but for this project, we will work with OHLC and Weighted_price because of their very high inter-correlation. We will henceforth refer to OHLC and Weighted_price as\u00a0OHLCWp.</p>\n<pre><em># selecting OHLCWp features as our working dataframe<br></em>daily_OHLCWp = daily_df_imputed.iloc[:, [0, 1, 2, 3, 6]].copy()</pre>\n<h3>Stationarity Test Using the ADF\u00a0Test</h3>\n<a href=\"https://medium.com/media/efadff68f16a98bcd858fa8014129636/href\">https://medium.com/media/efadff68f16a98bcd858fa8014129636/href</a><pre>get_adf_results(daily_OHLCWp)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/589/1*SIjUFvdEAjYX7LehfJF9JA.png\"></figure><p>From the results, we see that all the features are not stationary because the ADF statistics are all greater than those at the 10% threshold.</p>\n<h3>Johansen\u2019s Co-Integration Test</h3>\n<a href=\"https://medium.com/media/38d51fa4d3f553324cdff37bf2db11ea/href\">https://medium.com/media/38d51fa4d3f553324cdff37bf2db11ea/href</a><pre><em># getting the cointegration results of OHLCWp<br></em>coint_result_OHLCWp = get_coint_results(daily_OHLCWp)</pre>\n<pre>coint_result_OHLCWp[0] <em># displays the dataframe of cointegration <br># results</em></pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/649/1*8JoS0r6ODj7JHc3w0yGa2Q.png\"></figure><p>From the results, we see that OHLCWp is not co-integrating even to a threshold of 90% because the last Trace and maximum Eigen statistics are lesser than those at the 90% threshold. We will then check if the log transform of OHLCWp is stationary.</p>\n<pre>get_adf_results(np.log(daily_OHLCWp))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/581/1*E4VJcP3FjBwLdOqcRfRIlg.png\"></figure><p>From the results, we see that the log transform of OHLCWp is not stationary also. We will then check if the log transform of OHLCWp is co-integrating.</p>\n<pre>coint_result_log_OHLCWp = get_coint_results(np.log(daily_OHLCWp))<br>coint_result_log_OHLCWp[0]</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/647/1*X8q6Qb8vgO6WXckJzaux-A.png\"></figure><p>From the results, we see that the log transform of OHLCWp is co-integrating to a threshold of 95% because the last trace and maximum Eigen statistics are greater than those at the 95% threshold. So, there exists an eigenvector such that the matrix multiplication (linear transformation) of OHLCWp by the eigenvector will result in a stationary series even though each of OHLCWp is not stationary. Let\u2019s check it\u00a0out.</p>\n<pre><em># operating the non-stationary OHLC on the eigen vectors<br></em>stationary_OHLCWp = np.matmul(np.array(np.log(daily_OHLCWp)), coint_result_log_OHLCWp[1].reshape(-1, 1))</pre>\n<pre>plt.plot(daily_OHLCWp.index, stationary_OHLCWp)<br>plt.xlabel('years')<br>plt.ylabel('Co-integrated OHLCWp')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/398/1*dysfKr-v20vZTO1Le3ynYQ.png\"></figure><p>The resulting time series looks stationary. Let\u2019s further confirm this by conducting the ADF\u00a0test.</p>\n<pre>get_adf_results(pd.DataFrame({'OHLCWp': stationary_OHLCWp.flatten()}, index=daily_df_imputed.index))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/586/1*a8YGVX6Uch2eR9rlx4ilsg.png\"></figure><h3>Granger\u2019s Causation Test</h3>\n<a href=\"https://medium.com/media/2b263472a1ee92c3d6345f81301d1f5b/href\">https://medium.com/media/2b263472a1ee92c3d6345f81301d1f5b/href</a><p>Splitting the dataset into train and test\u00a0set</p>\n<a href=\"https://medium.com/media/0a7dc5b2897d18226a8d0604bb1db7e0/href\">https://medium.com/media/0a7dc5b2897d18226a8d0604bb1db7e0/href</a><pre><em># splitting OHLCWp into train and test set (25%)<br></em>OHLCWp_train, OHLCWp_test = ts_train_test_split(np.log(daily_OHLCWp), 0.25)</pre>\n<pre>from statsmodels.tsa.vector_ar.var_model import VAR <em># imports VAR</em><br>model_ohlcwp = VAR(OHLCWp_train) <em># initializes VAR object</em><br>model_ohlcwp.select_order(maxlags=20).summary() <em># gets the summary</em></pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/247/1*TVcMm_yx3VH4KNfMY6RGmw.png\"></figure><p>From the model summary above, the AIC score increases till the 11th lag beyond which it decreases. We then conclude that the best lag order to use for our model is 11. We will also use this lag for Granger\u2019s causality test.</p>\n<pre>get_grangers_causation_results(np.log(daily_OHLCWp), 11)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/458/1*EFeiH5jo3asrM_CiJK2HIw.png\"></figure><p>From the causation test, we see clearly that almost all the p-values of OHLCWp are less than the 0.05 significant threshold while just two of them are greater than 0.05 but less than 0.1 significant threshold. So, we can conclude that OHLCWp granger-cause one\u00a0another.</p>\n<h3>Modelling</h3>\n<p>For our model, we will be using walk-forward validation as our <a href=\"https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\">back-testing</a> scheme. Walk-forward validation is the gold standard of time series model evaluation and the state of the art k-fold cross-validation of the time\u00a0series.</p>\n<p>In walk-forward validation, after splitting our data into train and test set; we recursively do one-step prediction (predict the first value of our test set) and add the actual value of our prediction from the test set to the train\u00a0set.</p>\n<p>For our data, we will be using a test-size of 25%\u00a0(0.25).</p>\n<a href=\"https://medium.com/media/bc1348251ba267dfb7ba5ec2f9536031/href\">https://medium.com/media/bc1348251ba267dfb7ba5ec2f9536031/href</a><pre><em># walk forward for log of OHLCWp<br></em>OHLCWp_VAR_results_test, OHLCWp_VAR_results_pred = walk_forward_VAR(np.log(daily_OHLCWp), 0.25, 11)</pre>\n<pre><em># taking the inverse log (exp) of the walk forward OHLCWp results<br></em>OHLCWp_VAR_results = np.exp(OHLCWp_VAR_results_test), np.exp(OHLCWp_VAR_results_pred)</pre>\n<h3>Visualizing Results</h3>\n<a href=\"https://medium.com/media/f12dda52f16c0b93dc367c3cc8a1cb63/href\">https://medium.com/media/f12dda52f16c0b93dc367c3cc8a1cb63/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*B8r_g11kedcYHaKErF95RQ.png\"></figure><p>Let\u2019s have a look at the residual plot to see if there are any patterns a machine learning model can\u00a0pick.</p>\n<a href=\"https://medium.com/media/6fe38880fc9a51b2ebeb96daf58e5102/href\">https://medium.com/media/6fe38880fc9a51b2ebeb96daf58e5102/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*i_TkNugReo-nAOxqUkRzQQ.png\"></figure><p>From the plots, they are pretty no patterns left. To confirm this, we will look at the distribution plots (histogram) of the residuals. Normally, it ought to resemble a Gaussian distribution to confirm that the residuals are normally distributed random\u00a0noise.</p>\n<a href=\"https://medium.com/media/19bb38b0a3b2b3c0c2f748b0950b4e79/href\">https://medium.com/media/19bb38b0a3b2b3c0c2f748b0950b4e79/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RjYIVoaGV4GyablqAdpR4g.png\"></figure><p>From the plot, we see that the residuals are normally distributed.</p>\n<h3>Evaluation</h3>\n<p>As our evaluation metrics, we will be using root mean squared error (RMSE) and mean absolute percentage error\u00a0(MAPE).</p>\n<a href=\"https://medium.com/media/a0885a6e959f3e729caad119f3c580f5/href\">https://medium.com/media/a0885a6e959f3e729caad119f3c580f5/href</a><pre><em># getting the rmse and mape scores of OHLCWp<br></em>get_rmse_and_mape(OHLCWp_VAR_results[0], OHLCWp_VAR_results[1])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/259/1*HU9afaE16dWS5Rvr00o9Xw.png\"></figure><p>We see that the VAR model performs pretty good in modelling Bitcoin\u2019s OHLCWp. Our model performs best in modelling Bitcoin\u2019 Open, then Weighted_Price, followed by High, Low and\u00a0Close.</p>\n<h3>Conclusion</h3>\n<p>We have seen how to use VAR to model Bitcoin\u2019s OHLCWp using python. After imputing Nan values, we resampled to a daily timeframe, imputed time jumps, carried out correlation, ADF, Johansen\u2019s co-integration, and Granger\u2019s causality test. We then used a walk-forward scheme to back-test the VAR model on the last 25% of Bitcoin\u2019s OHLCWp from our data and got our RMSE and MAPE scores. See the complete notebook\u00a0<a href=\"https://www.kaggle.com/jolomitosanwumi/notebookb14540ce86\">here</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6cae0327ac30\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/hamoye-blogs/using-vector-auto-regression-to-model-bitcoin-prices-6cae0327ac30\">Using Vector Auto-Regression to Model Bitcoin Prices</a> was originally published in <a href=\"https://medium.com/hamoye-blogs\">Hamoye Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Implementing multivariate time series in\u00a0Python</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SN3tfnqnqf_Dc-2wY13-8A.jpeg\"><figcaption>Photo from Unsplash by Andre Francois\u00a0McKenzie</figcaption></figure><p>As our stage E project in the <a href=\"https://hamoye.com/\">Hamoye</a> data science internship, my group forecasted Bitcoin\u2019s prices. In this article, I will be sharing the approach I used in forecasting Bitcoin\u2019s Open, High, Low, Close (OHLC), and Weighed price. The <a href=\"https://www.kaggle.com/mczielinski/bitcoin-historical-data\">dataset</a> is a minute sampling of Bitcoin\u2019s historical data from 31st December 2011 to 14th September 2020 available on\u00a0Kaggle.</p>\n<p>Vector Auto-Regression (VAR) is the multivariate version of Auto-Regression (AR). Unlike the AR model which represents the future value of a time series as a function of its past values (lag values of itself), the VAR model represents the future value of a time series as a function of its past values and those of other related time series. It is more advantageous as it considers the interdependencies amongst different time\u00a0series.</p>\n<p>In this article, we will be using VAR to model Bitcoin\u2019s OHLC and Weighted price. We will assume you are familiar with the basics of time series, <a href=\"https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\">Augmented Dickey-Fuller (ADF) test</a>, <a href=\"https://en.wikipedia.org/wiki/Johansen_test\">Johansen\u2019s co-integration test</a>, and <a href=\"https://en.wikipedia.org/wiki/Granger_causality\">Granger\u2019s causality test</a>.</p>\n<h3>Loading in the\u00a0Data</h3>\n<pre><em># importing useful libraries<br></em>import numpy as np<br>import pandas as pd<br>import matplotlib.pyplot as plt<br>import seaborn as sns </pre>\n<pre>import warnings<br>warnings.filterwarnings('ignore') </pre>\n<pre>%matplotlib inline</pre>\n<pre><em># loading in the data<br></em>df = pd.read_csv('../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv')<br>print(df.shape) <em># prints \"(4572257, 8)\"</em></pre>\n<pre>bitcoin_df = df.copy() <em># making a copy of the dataset to work with</em><br>df.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/582/1*Hr8_DXxqhhxJbvXNslWOaw.png\"></figure><h3>Data Visualization</h3>\n<a href=\"https://medium.com/media/0bc7fee8c59c233cc9c1688f292d3dfd/href\">https://medium.com/media/0bc7fee8c59c233cc9c1688f292d3dfd/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1ET_TLEJKlkAPlDNQVrHdg.png\"></figure><h3>Preprocessing</h3>\n<pre><em># converting the 'Timestamp' column to datetime object<br></em>time_col = pd.to_datetime(bitcoin_df['Timestamp'], unit='s') </pre>\n<pre><em># dropping the 'Timestamp' column <br></em>bitcoin_df.drop('Timestamp', axis=1, inplace=True) </pre>\n<pre><em># creating a new 'Timestamp' column with datetime dtype<br></em>bitcoin_df['Timestamp'] = time_col  </pre>\n<pre><em># makes 'Timestamp' the index<br></em>bitcoin_df.set_index('Timestamp', inplace=True) </pre>\n<pre><em># creating a dataframe with boolean values indicating the presence<br># of missing values<br></em>missing_val_mask = bitcoin_df.isnull()<br>missing_val_mask.sum()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/235/1*n-ybskejXgvw0wH5lRZxZQ.png\"></figure><p>Let\u2019s check if all columns have Nan values for each row with any Nan\u00a0value.</p>\n<pre>print(missing_val_mask.sum(axis=1).sum() == len(bitcoin_df.columns) * missing_val_mask.sum().loc['Open']) </pre>\n<pre><em># prints \"True\"</em></pre>\n<p>So, we have confirmed that all columns have Nan values for each row with any Nan value. Now, we check the number of rows with missing\u00a0values.</p>\n<pre>n_missing_val = missing_val_mask.sum().loc['Open'] <br>print('There are {} rows with missing values which make up {}% of the total number of rows'.format(n_missing_val, round(100*n_missing_val / bitcoin_df.shape[0], 3)))</pre>\n<pre><em># prints \"There are 1241716 rows with missing values which make up 27.158% of the total number of rows\"</em></pre>\n<p>Let\u2019s see the reason for the missing values as stated by Zielak\u200a\u2014<em>\u200athe provider of the dataset</em>. This will make us know how to handle\u00a0them.</p>\n<blockquote>\n<em>\u2026</em><strong><em>TIMESTAMPS WITHOUT ANY TRADES OR ACTIVITY HAVE THEIR DATA FIELDS FILLED WITH\u00a0NANS</em></strong><em>\u2026</em>\n</blockquote>\n<p>Zielak also told us about jumps in timestamps and here is the\u00a0reason:</p>\n<blockquote>\n<strong><em>\u2026IF A TIMESTAMP IS MISSING, OR IF THERE ARE JUMPS, THIS MAY BE BECAUSE THE EXCHANGE (OR ITS API) WAS DOWN, THE EXCHANGE (OR ITS API) DID NOT EXIST, OR SOME OTHER UNFORESEEN TECHNICAL ERROR IN THE DATA REPORTING OR GATHERING</em></strong>.</blockquote>\n<p>Since the cause of <em>missingness</em> is \u201cno trading activity\u201d, we will use any of the following two ways to handle the missing\u00a0values:</p>\n<ol>\n<li>By imputing Volume_(BTC) and Volume_(Currency)with zeros since there was no trading activity. Then, we do forward fill for Close and impute Open, High, Low and Weighted_Price with the values ofClose so the Bitcoin chart will be a horizontal line for that period of inactivity.</li>\n<li>Dropping rows with Nan values because there were no trades for those\u00a0periods.</li>\n</ol>\n<p>We will be using the first approach to build our model because we know the reason for <em>missingness </em>and the percentage of Nan values is\u00a027.158%.</p>\n<a href=\"https://medium.com/media/f6bc13a26b0f687c861a8d5baab425fd/href\">https://medium.com/media/f6bc13a26b0f687c861a8d5baab425fd/href</a><pre>bitcoin_df_imputed = impute(bitcoin_df) # imputes 'bitcoin_df'</pre>\n<pre><em># checking for Nan values after imputing<br></em>print(bitcoin_df_imputed.isnull().sum().sum()) <br><em># prints \"0\"</em></pre>\n<pre>bitcoin_df_imputed.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*EWr25nj_N_Q8OXys_FnHfQ.png\"></figure><h3>Resampling</h3>\n<p>We will be down-sampling to a daily frequency. After resampling, we will be using first, max, min, last as aggregation functions for each of OHLC respectively. We will then use sum as those of Volume(BTC) and Volume(Currency\u00a0. Weighted_Price is the ratio of Volume(Currency) to Volume(BTC)\u00a0.</p>\n<pre>daily_Open = bitcoin_df_imputed['Open'].resample('D').first()<br>daily_High = bitcoin_df_imputed['High'].resample('D').max()<br>daily_Low = bitcoin_df_imputed['Low'].resample('D').min()<br>daily_Close = bitcoin_df_imputed['Close'].resample('D').last()<br>daily_Volume_BTC = bitcoin_df_imputed['Volume_(BTC)'].resample('D').sum()<br>daily_Volume_Currency = bitcoin_df_imputed['Volume_(Currency)'].resample('D').sum()</pre>\n<pre><em># making a dataframe of the resampled time series<br></em>daily_df = pd.DataFrame({'Open': daily_Open, 'High': daily_High, 'Low': daily_Low, 'Close': daily_Close, 'Volume_(BTC)': daily_Volume_BTC, 'Volume_(Currency)': daily_Volume_Currency})</pre>\n<pre><em># checking for Nan values after resampling.<br></em>daily_df.isnull().sum().sum() <em># prints 12</em></pre>\n<p>After resampling, we see that OHLC has Nan values in them. This is because of the presence of time jumps in the data. We will impute them using the same approach as\u00a0before.</p>\n<pre>daily_df_imputed = impute(daily_df)<br>print('bitcoin_df_imputed has been downsampled from a minute timeframe of {}'.format(bitcoin_df_imputed.shape[0])\\<br>+ ' observations to a daily timeframe of {} observations'.format( \\ daily_df_imputed.shape[0]))</pre>\n<pre><em># prints \"bitcoin_df_imputed has been downsampled from a minute timeframe of 4572257 observations to a daily timeframe of 3181 observations\"</em></pre>\n<p>Let\u2019s check the number of observations in the last day of our actual\u00a0dataset.</p>\n<pre>bitcoin_df.tail()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/709/1*NfG2ju2Bsvq5zDEUbjuZ1g.png\"></figure><p>Since the actual data ended on 2020\u201309\u201314 00:00:00, we will do away with the last observation because that day has only one observation in the minute time\u00a0frame.</p>\n<pre>daily_df_imputed = daily_df_imputed.iloc[:-1].copy()<br>daily_df_imputed.tail()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/650/1*50pPcZd0cVbgHGsLBIVhrw.png\"></figure><p>Let\u2019s check the plots of the daily resampled series time\u00a0series.</p>\n<pre>plot_series_of_each_column(daily_df_imputed)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/811/1*0LsJHvNQTYnhjXRZg3Wyyg.png\"></figure><h3>Correlation Test</h3>\n<pre><em># correlation heatmap</em></pre>\n<pre><em># masking out the upper-traingular matrix<br></em>mask = np.triu(daily_df_imputed.corr())</pre>\n<pre>plt.figure(figsize=(7, 7)) <br>sns.heatmap(daily_df_imputed.corr(), mask=mask, xticklabels=True, yticklabels=True, cmap='coolwarm', annot=True)<br><em># xticklabels and yticklabels are set to True to display all columns # in the heatmap<br></em>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/490/1*J5PP0wlbPlOP0q_uj4mc1g.png\"></figure><p>From the correlation heat-map, we see that OHLC and Weighted_price are highly correlated therefore, we will group them as our multivariate time series. We could also include Volume(Currency) but for this project, we will work with OHLC and Weighted_price because of their very high inter-correlation. We will henceforth refer to OHLC and Weighted_price as\u00a0OHLCWp.</p>\n<pre><em># selecting OHLCWp features as our working dataframe<br></em>daily_OHLCWp = daily_df_imputed.iloc[:, [0, 1, 2, 3, 6]].copy()</pre>\n<h3>Stationarity Test Using the ADF\u00a0Test</h3>\n<a href=\"https://medium.com/media/efadff68f16a98bcd858fa8014129636/href\">https://medium.com/media/efadff68f16a98bcd858fa8014129636/href</a><pre>get_adf_results(daily_OHLCWp)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/589/1*SIjUFvdEAjYX7LehfJF9JA.png\"></figure><p>From the results, we see that all the features are not stationary because the ADF statistics are all greater than those at the 10% threshold.</p>\n<h3>Johansen\u2019s Co-Integration Test</h3>\n<a href=\"https://medium.com/media/38d51fa4d3f553324cdff37bf2db11ea/href\">https://medium.com/media/38d51fa4d3f553324cdff37bf2db11ea/href</a><pre><em># getting the cointegration results of OHLCWp<br></em>coint_result_OHLCWp = get_coint_results(daily_OHLCWp)</pre>\n<pre>coint_result_OHLCWp[0] <em># displays the dataframe of cointegration <br># results</em></pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/649/1*8JoS0r6ODj7JHc3w0yGa2Q.png\"></figure><p>From the results, we see that OHLCWp is not co-integrating even to a threshold of 90% because the last Trace and maximum Eigen statistics are lesser than those at the 90% threshold. We will then check if the log transform of OHLCWp is stationary.</p>\n<pre>get_adf_results(np.log(daily_OHLCWp))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/581/1*E4VJcP3FjBwLdOqcRfRIlg.png\"></figure><p>From the results, we see that the log transform of OHLCWp is not stationary also. We will then check if the log transform of OHLCWp is co-integrating.</p>\n<pre>coint_result_log_OHLCWp = get_coint_results(np.log(daily_OHLCWp))<br>coint_result_log_OHLCWp[0]</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/647/1*X8q6Qb8vgO6WXckJzaux-A.png\"></figure><p>From the results, we see that the log transform of OHLCWp is co-integrating to a threshold of 95% because the last trace and maximum Eigen statistics are greater than those at the 95% threshold. So, there exists an eigenvector such that the matrix multiplication (linear transformation) of OHLCWp by the eigenvector will result in a stationary series even though each of OHLCWp is not stationary. Let\u2019s check it\u00a0out.</p>\n<pre><em># operating the non-stationary OHLC on the eigen vectors<br></em>stationary_OHLCWp = np.matmul(np.array(np.log(daily_OHLCWp)), coint_result_log_OHLCWp[1].reshape(-1, 1))</pre>\n<pre>plt.plot(daily_OHLCWp.index, stationary_OHLCWp)<br>plt.xlabel('years')<br>plt.ylabel('Co-integrated OHLCWp')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/398/1*dysfKr-v20vZTO1Le3ynYQ.png\"></figure><p>The resulting time series looks stationary. Let\u2019s further confirm this by conducting the ADF\u00a0test.</p>\n<pre>get_adf_results(pd.DataFrame({'OHLCWp': stationary_OHLCWp.flatten()}, index=daily_df_imputed.index))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/586/1*a8YGVX6Uch2eR9rlx4ilsg.png\"></figure><h3>Granger\u2019s Causation Test</h3>\n<a href=\"https://medium.com/media/2b263472a1ee92c3d6345f81301d1f5b/href\">https://medium.com/media/2b263472a1ee92c3d6345f81301d1f5b/href</a><p>Splitting the dataset into train and test\u00a0set</p>\n<a href=\"https://medium.com/media/0a7dc5b2897d18226a8d0604bb1db7e0/href\">https://medium.com/media/0a7dc5b2897d18226a8d0604bb1db7e0/href</a><pre><em># splitting OHLCWp into train and test set (25%)<br></em>OHLCWp_train, OHLCWp_test = ts_train_test_split(np.log(daily_OHLCWp), 0.25)</pre>\n<pre>from statsmodels.tsa.vector_ar.var_model import VAR <em># imports VAR</em><br>model_ohlcwp = VAR(OHLCWp_train) <em># initializes VAR object</em><br>model_ohlcwp.select_order(maxlags=20).summary() <em># gets the summary</em></pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/247/1*TVcMm_yx3VH4KNfMY6RGmw.png\"></figure><p>From the model summary above, the AIC score increases till the 11th lag beyond which it decreases. We then conclude that the best lag order to use for our model is 11. We will also use this lag for Granger\u2019s causality test.</p>\n<pre>get_grangers_causation_results(np.log(daily_OHLCWp), 11)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/458/1*EFeiH5jo3asrM_CiJK2HIw.png\"></figure><p>From the causation test, we see clearly that almost all the p-values of OHLCWp are less than the 0.05 significant threshold while just two of them are greater than 0.05 but less than 0.1 significant threshold. So, we can conclude that OHLCWp granger-cause one\u00a0another.</p>\n<h3>Modelling</h3>\n<p>For our model, we will be using walk-forward validation as our <a href=\"https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\">back-testing</a> scheme. Walk-forward validation is the gold standard of time series model evaluation and the state of the art k-fold cross-validation of the time\u00a0series.</p>\n<p>In walk-forward validation, after splitting our data into train and test set; we recursively do one-step prediction (predict the first value of our test set) and add the actual value of our prediction from the test set to the train\u00a0set.</p>\n<p>For our data, we will be using a test-size of 25%\u00a0(0.25).</p>\n<a href=\"https://medium.com/media/bc1348251ba267dfb7ba5ec2f9536031/href\">https://medium.com/media/bc1348251ba267dfb7ba5ec2f9536031/href</a><pre><em># walk forward for log of OHLCWp<br></em>OHLCWp_VAR_results_test, OHLCWp_VAR_results_pred = walk_forward_VAR(np.log(daily_OHLCWp), 0.25, 11)</pre>\n<pre><em># taking the inverse log (exp) of the walk forward OHLCWp results<br></em>OHLCWp_VAR_results = np.exp(OHLCWp_VAR_results_test), np.exp(OHLCWp_VAR_results_pred)</pre>\n<h3>Visualizing Results</h3>\n<a href=\"https://medium.com/media/f12dda52f16c0b93dc367c3cc8a1cb63/href\">https://medium.com/media/f12dda52f16c0b93dc367c3cc8a1cb63/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*B8r_g11kedcYHaKErF95RQ.png\"></figure><p>Let\u2019s have a look at the residual plot to see if there are any patterns a machine learning model can\u00a0pick.</p>\n<a href=\"https://medium.com/media/6fe38880fc9a51b2ebeb96daf58e5102/href\">https://medium.com/media/6fe38880fc9a51b2ebeb96daf58e5102/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*i_TkNugReo-nAOxqUkRzQQ.png\"></figure><p>From the plots, they are pretty no patterns left. To confirm this, we will look at the distribution plots (histogram) of the residuals. Normally, it ought to resemble a Gaussian distribution to confirm that the residuals are normally distributed random\u00a0noise.</p>\n<a href=\"https://medium.com/media/19bb38b0a3b2b3c0c2f748b0950b4e79/href\">https://medium.com/media/19bb38b0a3b2b3c0c2f748b0950b4e79/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RjYIVoaGV4GyablqAdpR4g.png\"></figure><p>From the plot, we see that the residuals are normally distributed.</p>\n<h3>Evaluation</h3>\n<p>As our evaluation metrics, we will be using root mean squared error (RMSE) and mean absolute percentage error\u00a0(MAPE).</p>\n<a href=\"https://medium.com/media/a0885a6e959f3e729caad119f3c580f5/href\">https://medium.com/media/a0885a6e959f3e729caad119f3c580f5/href</a><pre><em># getting the rmse and mape scores of OHLCWp<br></em>get_rmse_and_mape(OHLCWp_VAR_results[0], OHLCWp_VAR_results[1])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/259/1*HU9afaE16dWS5Rvr00o9Xw.png\"></figure><p>We see that the VAR model performs pretty good in modelling Bitcoin\u2019s OHLCWp. Our model performs best in modelling Bitcoin\u2019 Open, then Weighted_Price, followed by High, Low and\u00a0Close.</p>\n<h3>Conclusion</h3>\n<p>We have seen how to use VAR to model Bitcoin\u2019s OHLCWp using python. After imputing Nan values, we resampled to a daily timeframe, imputed time jumps, carried out correlation, ADF, Johansen\u2019s co-integration, and Granger\u2019s causality test. We then used a walk-forward scheme to back-test the VAR model on the last 25% of Bitcoin\u2019s OHLCWp from our data and got our RMSE and MAPE scores. See the complete notebook\u00a0<a href=\"https://www.kaggle.com/jolomitosanwumi/notebookb14540ce86\">here</a>.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6cae0327ac30\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/hamoye-blogs/using-vector-auto-regression-to-model-bitcoin-prices-6cae0327ac30\">Using Vector Auto-Regression to Model Bitcoin Prices</a> was originally published in <a href=\"https://medium.com/hamoye-blogs\">Hamoye Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["time-series-forecasting","hamoye-features","var-model","bitcoin-ohlc","multivariate-time-series"]}]}